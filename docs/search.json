[
  {
    "objectID": "posts/perceptron/perceptron_blog.html",
    "href": "posts/perceptron/perceptron_blog.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "source code link: https://github.com/yanruoz/yanruoz.github.io/blob/main/posts/perceptron/perceptron.py"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#implementation-of-the-perceptron-update",
    "href": "posts/perceptron/perceptron_blog.html#implementation-of-the-perceptron-update",
    "title": "Perceptron Blog",
    "section": "Implementation of the Perceptron Update",
    "text": "Implementation of the Perceptron Update\nIn the fit() function, I first reset history as an empty array to avoid accumulating histories of multiple datasets. Then, I unpack the tuple returned by X.shape by assigning the number of data points to n, and the number of features to p. Then, I initialize a random initial weight vector w_tilt(0) as a 1D array composed of floats \\(\\in [1, -1]\\). Next, I modify the input X into X_ (which contains a column of 1s and corresponds to X_tilde).\nWithin the for loop that iterates within the range of the max steps input by the user, I generate a random number within the amount of data points (n). We have the equation for the perceptron update: \\(\\tilde{w}^{(t+1)}=\\tilde{w}^{(t)}+ \\mathbb{1}{(\\tilde{y_i} \\langle \\tilde{w}^{(t)}, \\tilde{x_i} \\rangle <0)}\\tilde{y_i} \\tilde{x_i}\\), where \\(\\tilde{y_i} = 2y_i-1\\) to make \\(\\tilde{y_i}\\) take the value of (-1 and 1) instead of (0 and 1). To implement it, I calculate different parts of the equation respectively and update the weight by multiplying them:\n---\ny_tilde_i = 2 * y[i] - 1  \nx_tilde_i = X_[i]\ny_tilde_pred = int(y_tilde_i * np.dot(x_tilde_i, self.w) < 0)\nself.w += y_tilde_pred * y_tilde_i * x_tilde_i\n---\n\n%load_ext autoreload\n%autoreload 2\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport random\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_moons"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#experiment",
    "href": "posts/perceptron/perceptron_blog.html#experiment",
    "title": "Perceptron Blog",
    "section": "Experiment",
    "text": "Experiment\n\nExperiment 1: Linearly-separable 2d data\nUsing a 2d dataset in which the data is linearly separable, the perceptron algorithm converges to weight vector \\(\\tilde{w}\\), which is the separating line shown in Fig 1.1.\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = n, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nplt.title(\"Fig 1.1: Linearly separable 2d data points\")\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThe evolution of the accuracy over training is shown in the printed history and Fig 1.2; the final accuracy of 1 confirms that the perceptron algorithm converges.\n\nnp.random.seed(12345)\np = Perceptron()\np.fit(X, y, max_steps = 10000)\np.w\nprint(p.history[-10:])\nfig = plt.plot(p.history)\nplt.title(\"Fig 1.2: Evolution of the accuracy over training of linearly separable 2d data\")\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\n\n\n\n\nnp.random.seed(12345)\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nplt.title(\"Fig 1.3: Linearly separable 2d data points with the separating line\")\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\np.score(X, y)\n\n1.0\n\n\n\n\n\n\n\nExperiment 2: Non-linearly-separable 2d data\nIn the case of not linearly separable 2d data, we generate such a dataset using make_moons function. As shown in the Fig 2.1, the data overlaps in a way that is not linearly separable.\n\nnp.random.seed(123)\n# Generate a non-linearly separable dataset with 100 samples\nX, y = make_moons(n_samples=100, noise=0.1, random_state=42)\n\n# Plot the dataset\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\nplt.title(\"Fig 2.1: Non-linearly-separable 2d data points\")\nplt.show()\n\n\n\n\nThe evolution of accuracy over training also shows that the algorithm does not converge when the iteration of max steps is completed.\n\nnp.random.seed(123)\np = Perceptron()\np.fit(X, y, max_steps = 1000)\np.w\nprint(p.history[-10:])\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n[0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86]\n\n\n\n\n\nThe separating line in the final iteration is shown in Fig.2.3, which does not separate the data obviously.\n\nnp.random.seed(123)\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nplt.title(\"Fig 2.3: Non-linearly-separable 2d data points with the separating line\")\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\np.score(X, y)\n\n0.86\n\n\n\n\n\n\n\nExperiment 3: 5d data\nThe perceptron algorithm is also able to work in more than 2 dimensions. In this experiment, the dataset has 5 features. The evolution of the accuracy over training is shown in Fig 3. Since the final score is less than 1, the algorithm does not converge; therefore, the data is not linearly separable.\n\nnp.random.seed(230230)\n\nn = 100\np_features = 5\n\nX, y = make_blobs(n_samples = n, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\np.w\nprint(p.history[-10:])\nfig = plt.plot(p.history)\nplt.title(\"Fig 1.2: Evolution of the accuracy over training of 5d data\")\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n[0.99, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98]"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#thinking-question",
    "href": "posts/perceptron/perceptron_blog.html#thinking-question",
    "title": "Perceptron Blog",
    "section": "Thinking Question",
    "text": "Thinking Question\nQ: What is the runtime complexity of a single iteration of the perceptron algorithm update as described by Equation 1? Assume that the relevant operations are addition and multiplication. Does the runtime complexity depend on the number of data points ? What about the number of features?\nR: The runtime complexity of a single iteration is \\(O(N)\\) because when calculating the dot product between the weight and the feature vector, we only consider one data point at a time. Therefore, the runtime complexity only depends on the number of features and not on the number of data points in a single iteration."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "This blog post demonstrates the motivation, methods, and results of our final project in speech emotion recognition.\n\n\n\n\n\n\nMay 14, 2023\n\n\nAlice (Yanruo) Zhang, Wen (Diana) Xu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog post, I introduce a Middlebury guest speaker Dr. Timnit Gebru, propose some questions for her, and reflect on her broad talk.\n\n\n\n\n\n\nApr 17, 2023\n\n\nAlice (Yanruo) Zhang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThe first part of the blog shows the implementation of least-squares linear regression in two ways: 1) the analytical way of using a formula for the optimal weight vector and 2) the way of using gradient descent. The second part performs experiments to illustrate the effect of overfitting. The last part demonstrates the LASSO regularization, an alternative algorithm that uses a modified loss function with a regularization term, for overparameterized problems.\n\n\n\n\n\n\nApr 3, 2023\n\n\nAlice (Yanruo) Zhang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis blog shows the standard workflow of machine learning through the classification of three penguine species. The primary goal is to determine the smallest number of measurements necessary to confidently determine the species.\n\n\n\n\n\n\nMar 9, 2023\n\n\nAlice (Yanruo) Zhang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nRegular and stochastic logistic regression fit models with experiements on learning rate, batch size, momentum, and feature numbers.\n\n\n\n\n\n\nMar 6, 2023\n\n\nAlice (Yanruo) Zhang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplementation of perceptron algorithm.\n\n\n\n\n\n\nFeb 22, 2023\n\n\nAlice (Yanruo) Zhang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/logistic_regression/gradient_blog.html",
    "href": "posts/logistic_regression/gradient_blog.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "Algorithm Implementation\nFirst, I implement the regular gradient descent model to calculate the loss function. In the fit() function, I first reset the history and obtain the data points and the number of features from the given dataset. Then, I initialized a random initial weight vector. In the main loop, the gradient is calculated for computing the loss and updating the weight. When the losses become indistinguishable, we consider finding the minimalized loss and thus terminate the loop.\nBased on the regular gradient descent model, we implement one of its key variants, the stochastic gradient descent with an optional momentum feature. In the fit_stochasitic() function, I compute the stochastic gradient by picking a random subset and computing the corresponding gradient. Specifically, we use a nested for loop. The outer loop iterates through the max number of epochs input by the user by shuffling them first. The inner loop iterates through arrays of equal size indicated by the user. Each array is a batch. Within each batch, we calculate the stochastic gradient and conduct the update.\n\n\nPerformance Check\nInitiate autoreload and import packages.\n\n%load_ext autoreload\n%autoreload 2\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport random\n\nfrom gradient import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}\n\n\nGenerate a 2D dataset.\n\nnp.random.seed(300)\n# make the data\nX, y = make_blobs(n_samples = 200, n_features = 2, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nFit the model for regular gradient and check history.\n\n# fit the model\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.01, max_epochs = 1000)\n\n# inspect the fitted value of w\nLR.w \n\n# check history\nprint(f\"{LR.score_history[-10:] = }\")\nprint(f\"{LR.loss_history[-10:] = }\")\n\nLR.score_history[-10:] = [0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925]\nLR.loss_history[-10:] = [0.22110667250928606, 0.22106582072749664, 0.22102504977749868, 0.22098435942579706, 0.22094374943980186, 0.2209032195878236, 0.22086276963906903, 0.2208223993636367, 0.22078210853251268, 0.22074189691756615]\n\n\nSeems right. Now we fit the models for stochasitc gradient (with momentum), stochasitc gradient (no momentum), and regular gradient. Then, we plot all three on the loglog plot to see how they converge over iterations.\n\nnp.random.seed(666)\n# stochasitc gradient (with momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = .05) \n\n# check history\nprint(f\"{LR.loss_history[-10:] = }\")\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\n# stochasitc gradient (no momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .05)\n# check history\nprint(f\"{LR.loss_history[-10:] = }\")\n\n# plot\nnum_steps = len(LR.loss_history)\nprint(num_steps)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n# gradient\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 100)\n\n# check history\nprint(f\"{LR.loss_history[-10:] = }\")\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\nprint(f\"{num_steps = }\")\n\n# plot loglog\nplt.loglog()\n\nlegend = plt.legend()\nplt.xlabel(\"Number of steps\")\nplt.ylabel(\"Loss\")\n\nLR.loss_history[-10:] = [0.1891885675258715, 0.18918135337947276, 0.1891732709523959, 0.1891664490195526, 0.18916397195720727, 0.18915691574954063, 0.1891532721564178, 0.18914877910489547, 0.1891452449340078, 0.18914101648260434]\nLR.loss_history[-10:] = [0.1891892578397053, 0.1891862554287734, 0.1891776959845537, 0.18917244702257963, 0.18916378484579333, 0.18915910372864198, 0.18915339492464875, 0.18914785080072152, 0.18914212274131073, 0.18913854088299892]\n100\nLR.loss_history[-10:] = [0.2547344581927334, 0.25400204588594855, 0.253283648059068, 0.25257887196972795, 0.25188733932459734, 0.2512086856255283, 0.2505425595506183, 0.24988862236804774, 0.24924654738070146, 0.24861601939972083]\nnum_steps = 100\n\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\nAll three models converge given small enough learning rate. In general, the regular gradient model converges slower than the stochastic gradient models and might need more epochs to find a good solution.\n\n\nExperiments\nIt’s time to play with the parameters even more and see how the learning rate, batch size, momentum, and feature numbers!\n\nA case in which gradient descent does not converge to a minimizer because the learning rate is too large.\n\nWhen the learning rate (step size) is too large, as shown below, the model does not converge any more but oscillates instead.\n\nnp.random.seed(306)\n#gradient\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 100, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\nprint(f\"{num_steps = }\")\n\n# plot loglog\nplt.loglog()\n\nlegend = plt.legend()\nplt.xlabel(\"Number of steps\")\nplt.ylabel(\"Loss\")\n\nnum_steps = 100\n\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\nA case in which the choice of batch size influences how quickly the algorithm converges.\n\nThe smaller the batch, the more quickly the algorithm converges.\n\nnp.random.seed(307)\n# stochasitc gradient (no momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 5, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (batch size = 5)\")\n\n# stochasitc gradient (no momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\n#print(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (batch size = 10)\")\n\n# plot loglog\nplt.loglog()\n\nlegend = plt.legend()\nplt.xlabel(\"Number of steps\")\nplt.ylabel(\"Loss\")\n\n53\n100\n\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\nA case in which the use of momentum significantly speeds up convergence.\n\nI tried multiple random seeds, while sometimes the stochastic gradient models with and without momentum display similar converging rate, the one with momentum sometimes out perform the one without. Therefore, in general, we can conclude that the use of momentum significantly speeds up convergence.\n\nnp.random.seed(312)\n# make the data (10 features)\nX2, y2 = make_blobs(n_samples = 200, n_features = 2, centers = [(-1, -1), (1, 1)])\n\n# stochasitc gradient (with momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X2, y2, \n                  m_epochs = 100, \n                  momentum = True, \n                  batch_size = 4, \n                  alpha = .07) \n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\n# print(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\n# stochasitc gradient (no momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X2, y2, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 4, \n                  alpha = .07)\n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\n# print(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n# plot loglog\nplt.loglog()\n\nlegend = plt.legend()\nplt.xlabel(\"Number of steps\")\nplt.ylabel(\"Loss\")\n\n42\n36\n\n\nText(0, 0.5, 'Loss')"
  },
  {
    "objectID": "posts/penguin_classification/penguin.html",
    "href": "posts/penguin_classification/penguin.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "# Import\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom warnings import simplefilter\nfrom sklearn.exceptions import ConvergenceWarning\nsimplefilter(\"ignore\", category=ConvergenceWarning) # avoid warning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import GridSearchCV\nfrom matplotlib.patches import Patch\nfrom itertools import combinations\nimport seaborn as sns"
  },
  {
    "objectID": "posts/penguin_classification/penguin.html#data-inspection",
    "href": "posts/penguin_classification/penguin.html#data-inspection",
    "title": "Classifying Palmer Penguins",
    "section": "Data Inspection",
    "text": "Data Inspection\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN"
  },
  {
    "objectID": "posts/penguin_classification/penguin.html#data-prep",
    "href": "posts/penguin_classification/penguin.html#data-prep",
    "title": "Classifying Palmer Penguins",
    "section": "Data Prep",
    "text": "Data Prep\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train.head()\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n\n\n\n\ny_train\n\narray([2, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 2, 1, 2, 2, 0, 0, 1,\n       2, 2, 0, 2, 0, 1, 1, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n       1, 0, 2, 2, 2, 1, 1, 1, 2, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 1, 0, 0,\n       2, 0, 2, 2, 0, 2, 0, 0, 2, 1, 1, 2, 2, 0, 1, 2, 2, 2, 1, 0, 1, 0,\n       0, 0, 0, 1, 2, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 1, 0, 2, 0, 2, 0, 2,\n       0, 0, 2, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0, 2, 2,\n       0, 2, 2, 0, 1, 2, 1, 2, 0, 0, 0, 2, 0, 0, 1, 1, 0, 2, 1, 2, 2, 2,\n       2, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 2, 0, 2, 0, 1, 0, 1, 2, 1, 1, 1,\n       2, 0, 1, 0, 0, 1, 0, 0, 2, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 2, 1, 2,\n       0, 1, 2, 0, 2, 2, 1, 1, 0, 2, 0, 0, 2, 1, 0, 2, 2, 1, 1, 2, 2, 2,\n       0, 2, 2, 2, 2, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 1, 0, 0, 0, 0, 1, 2,\n       0, 1, 1, 0, 2, 0, 1, 1, 1, 0, 2, 0, 2, 2])"
  },
  {
    "objectID": "posts/penguin_classification/penguin.html#data-exploration",
    "href": "posts/penguin_classification/penguin.html#data-exploration",
    "title": "Classifying Palmer Penguins",
    "section": "Data Exploration",
    "text": "Data Exploration\nIn the following table, we group the data by sex and calculate the mean body mass for each sex. We can see from the result that the mean body mass of the male penguins is substantially higher than the mean body mass of the female penguins.\n\nX_train.groupby(['Sex_MALE'])[['Body Mass (g)']].aggregate([np.mean,len ]).round(2)\n\n\n\n\n\n  \n    \n      \n      Body Mass (g)\n    \n    \n      \n      mean\n      len\n    \n    \n      Sex_MALE\n      \n      \n    \n  \n  \n    \n      0\n      3823.21\n      126\n    \n    \n      1\n      4613.08\n      130\n    \n  \n\n\n\n\nIn the subsequent pair chart, we inspect the correlation between each pair of features. We can see that the culmen length, the culmen depth, the flipper length, and the body mass are in general positively correlated with each other. It also seems like if we were to differentiate the species, Delta 15 N would be a good measurement because the means of the three clusters are more distinctly separated compared to that separation in other features.\n\nsns.set_theme(style=\"ticks\")\nsns.pairplot(train, hue=\"Species\")\n\n<seaborn.axisgrid.PairGrid at 0x7f7f9fd9da00>"
  },
  {
    "objectID": "posts/penguin_classification/penguin.html#choosing-features",
    "href": "posts/penguin_classification/penguin.html#choosing-features",
    "title": "Classifying Palmer Penguins",
    "section": "Choosing Features",
    "text": "Choosing Features\nIn the following chunk of code, we loop through three models (logistic regression, support vector machine, and random forest) as well as the groups of features to determine what are the best three features (2 quantitative and 1 qualitative) to determine the species. The skeleton of the code is based on the blog instruction (https://middlebury-csci-0451.github.io/CSCI-0451/assignments/blog-posts/blog-post-penguins.html#your-challenge).\n\n# Scale the model to prevent warnings\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\n\n\n# Search for the best columns witht the largest cross validation score\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\n# Create a dictionary to store the models of interest\nmodels = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"SVC\": SVC(gamma = 0.01),\n    \"Random Forest\": RandomForestClassifier()  \n}\n\n# Initiate an empty array for the best cols, the score, and the best model\nbest_cols = None\nbest_score = 0\nbest_model = None\n\n# Main loop to iterate through the feature groups\nfor model_name, model in models.items():\n    print(f\"Evaluating {model_name}...\")\n    for qual in all_qual_cols: \n        qual_cols = [col for col in X_train.columns if qual in col]\n        for pair in combinations(all_quant_cols, 2):\n            cols =  list(pair) + qual_cols\n            # print(cols)\n            X_subset = X_train[cols]\n            scores = cross_val_score(model, X_subset, y_train, cv=5) # You can modify the cv parameter as needed\n            mean_score = scores.mean()\n            if mean_score > best_score:\n                best_cols = cols\n                best_score = mean_score\n                best_model = model_name\n            \n    print(f\"The best score for {model_name} was {best_score} and the best columns were {best_cols}\")\n    \nprint(f\"The best model was {best_model} with a score of {best_score} and the best columns were {best_cols}\")\n\nEvaluating Logistic Regression...\nThe best score for Logistic Regression was 0.996078431372549 and the best columns were ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nEvaluating SVC...\nThe best score for SVC was 0.996078431372549 and the best columns were ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nEvaluating Random Forest...\nThe best score for Random Forest was 0.996078431372549 and the best columns were ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nThe best model was Logistic Regression with a score of 0.996078431372549 and the best columns were ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']"
  },
  {
    "objectID": "posts/penguin_classification/penguin.html#model-choices",
    "href": "posts/penguin_classification/penguin.html#model-choices",
    "title": "Classifying Palmer Penguins",
    "section": "Model Choices",
    "text": "Model Choices\n\nLogistic Regression\n\n# Train/Fit & Calculate training score\nLR = LogisticRegression()\nLR.fit(X_train[best_cols], y_train)\nLR_train_score = LR.score(X_train[best_cols], y_train)\nprint(f\"{LR_train_score=}\")\n\nLR_train_score=1.0\n\n\n\n\nOther Models\n\nDecision Tree\n\n# Use cross validation to find the best depth and plug back to the \"Choosing Features\" chunk\nnp.random.seed(12345)\n\nfig, ax = plt.subplots(1)\n\nfor d in range(2, 5):\n    T = DecisionTreeClassifier(max_depth = d)\n    m = cross_val_score(T, X_train[cols], y_train, cv = 10).mean()\n    ax.scatter(d, m, color = \"black\")\n    # ax.scatter(d, T.score(X_test, y_test), color = \"firebrick\")\n\nlabs = ax.set(xlabel = \"Complexity (depth)\", ylabel = \"Performance (score)\")\n\n\n\n\n\n# Train/Fit & Calculate training score\nDTC = DecisionTreeClassifier(max_depth = 3)\nDTC.fit(X_train[best_cols], y_train)\nDTC_train_score = DTC.score(X_train[best_cols], y_train)\nprint(f\"{DTC_train_score=}\")\n\n# Show the decision tree plot\nplt.figure(figsize=(12,12))\np = plot_tree(DTC, feature_names = best_cols, filled = True, class_names = species, fontsize=8)\nplt.show()\n\nDTC_train_score=0.9921875\n\n\n\n\n\n\n\nRandom Forest\n\n# Train/Fit & Calculate training score\nrf = RandomForestClassifier()\nrf.fit(X_train[best_cols], y_train)\nrf_train_score = rf.score(X_train[best_cols], y_train)\nprint(f\"{rf_train_score=}\")\n\nrf_train_score=1.0\n\n\n\n\nSVC\n\n# Use grid search to find the best gamma and plug back to the \"Choosing Features\" chunk\nparam_grid = {'gamma': np.logspace(-5, 5, num=11)}\n\nsvc = SVC()\ngrid_search = GridSearchCV(svc, param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\n\nprint(\"Best gamma:\", grid_search.best_params_['gamma'])\nprint(\"Best score:\", grid_search.best_score_)\n\nBest gamma: 0.01\nBest score: 0.7730769230769232\n\n\n\n# Train/Fit & Calculate training score\nsvc_best = SVC(gamma = 0.01)\nsvc_best.fit(X_train[best_cols], y_train)\nsvc_train_score = svc_best.score(X_train[best_cols], y_train)\nprint(f\"{svc_train_score=}\")\n\nsvc_train_score=0.98046875"
  },
  {
    "objectID": "posts/penguin_classification/penguin.html#testing",
    "href": "posts/penguin_classification/penguin.html#testing",
    "title": "Classifying Palmer Penguins",
    "section": "Testing",
    "text": "Testing\n\n# Get the test data\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\n# LR\nLR.fit(X_train[best_cols], y_train)\nLR_score = LR.score(X_test[best_cols], y_test)\nprint(\"LR test score:\", LR_score)\n\n# Decision Tree\nDTC.fit(X_train[best_cols], y_train)\nDTC_score = DTC.score(X_test[best_cols], y_test)\nprint(\"Decision Tree test score:\", DTC_score)\n\n# Random Forest\nrf = RandomForestClassifier()\nrf.fit(X_train[best_cols], y_train)\nrf_score = rf.score(X_test[best_cols], y_test)\nprint(\"Random Forest test score:\", rf_score)\n\n# SVC\nsvc_best = SVC(gamma=0.01)\nsvc_best.fit(X_train[best_cols], y_train)\nsvc_score = svc_best.score(X_test[best_cols], y_test)\nprint(\"SVC test score:\", svc_score)\n\nLR test score: 1.0\nDecision Tree test score: 0.9852941176470589\nRandom Forest test score: 0.9852941176470589\nSVC test score: 0.9264705882352942"
  },
  {
    "objectID": "posts/penguin_classification/penguin.html#plotting-decision-regions",
    "href": "posts/penguin_classification/penguin.html#plotting-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "Plotting Decision Regions",
    "text": "Plotting Decision Regions\n\n# The skeleton of the code is based on the blog instruction (https://middlebury-csci-0451.github.io/CSCI-0451/assignments/blog-posts/blog-post-penguins.html#your-challenge).\ndef plot_regions(model, X, y, model_name):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    # add the title\n    fig.suptitle(model_name + \" Decision Boundaries\", fontsize=14)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n    \n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nDecision Boundaries for Training Data\n\nplot_regions(LR, X_train[best_cols], y_train, model_name = \"LR\")\nplot_regions(DTC, X_train[best_cols], y_train, model_name = \"DTC\")\nplot_regions(rf, X_train[best_cols], y_train, model_name = \"Random Forest\")\nplot_regions(svc_best, X_train[best_cols], y_train, model_name = \"Support Vector Machine\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Boundaries for Testing Data\n\nplot_regions(LR, X_test[best_cols], y_test, model_name = \"LR\")\nplot_regions(DTC, X_test[best_cols], y_test, model_name = \"DTC\")\nplot_regions(rf, X_test[best_cols], y_test, model_name = \"Random Forest\")\nplot_regions(svc_best, X_test[best_cols], y_test, model_name = \"Support Vector Machine\")"
  },
  {
    "objectID": "posts/linear_reg/linear_reg.html",
    "href": "posts/linear_reg/linear_reg.html",
    "title": "Implementing Linear Regression",
    "section": "",
    "text": "Link to the source code: https://github.com/yanruoz/yanruoz.github.io/blob/main/posts/linear_reg/linear_reg.py"
  },
  {
    "objectID": "posts/linear_reg/linear_reg.html#experiments",
    "href": "posts/linear_reg/linear_reg.html#experiments",
    "title": "Implementing Linear Regression",
    "section": "Experiments",
    "text": "Experiments\nIn the experiment below, we plot the change in the training and validation scores as the number of features increases and the number of training data points remains constant (100). We see that initially, the increase in feature number leads to a higher score overall. However, as the feature number increases to around 90, the score decreases sharply. As it approaches the number of training points, despite fluctuations, it follows a decreasing trend to negative numbers, indicating poor modeling. This phenomenon illustrates that overfitting might lead to worse performance.\n\n# create n_train and a list of p_features\nnp.random.seed(1234)\nn_train = 100\nn_val = 100\np_features_list = np.arange(1, n_train, 1).tolist() # step=1\n# print(p_features_list)\nnoise = 0.2\n\n# create empty lists to store training and validation scores\ntraining_score_list = []\nvalidation_score_list = []\n\nfor p_features in p_features_list:\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    \n    # fit the model\n    LR = LinearRegression()\n    LR.fit(X_train, y_train)\n    \n    # compute and append scores\n    training_score = LR.score(X_train, y_train).round(4)\n    validation_score = LR.score(X_val, y_val).round(4)\n    training_score_list.append(training_score)\n    validation_score_list.append(validation_score)\n\n# plot\nplt.plot(p_features_list, training_score_list, label = \"training score\")\nplt.plot(p_features_list, validation_score_list, label = \"validation score\")\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Score\")\nplt.legend(loc='best')\nplt.show\n\n<function matplotlib.pyplot.show(close=None, block=None)>"
  },
  {
    "objectID": "posts/linear_reg/linear_reg.html#lasso-regularization",
    "href": "posts/linear_reg/linear_reg.html#lasso-regularization",
    "title": "Implementing Linear Regression",
    "section": "LASSO Regularization",
    "text": "LASSO Regularization\n\nImplementation\n\n# use lasso regularization\nL = Lasso(alpha = 0.001)\n\n# fit the model on data\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL.fit(X_train, y_train)\n\nLasso(alpha=0.001)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=0.001)\n\n\n\n# compute the score\nL.score(X_val, y_val)\n\n0.7806576461432739\n\n\n\n\nExperiments\nWe implement the same experiments as previously did with linear regression, increasing the number of features up to or even past n_train - 1 (In this case, n_train=100, and I increased p_features to 150). The plots show the change of score as the number of features increases, with different values for the regularization strenth alpha in each subplot.\n\n# create n_train and a list of p_features\nnp.random.seed(1234)\nn_train = 100\nn_val = 100\np_features_list = np.arange(1, n_train + 50, 1).tolist()\n# print(p_features_list)\nnoise = 0.2\n\n\n# set subplot parameters\nfig, axarr = plt.subplots(2, 2)\nfig.suptitle('Comparison between different alphas in LASSO regularization')\nplt.rcParams[\"figure.figsize\"] = (10, 4)\n\n\n# initiate alpha list & axis array list\nalpha_list = [0.1, 0.01, 0.001, 0.0001]\naxarr_list = [(0,0), (0,1), (1,0), (1,1)]\n\n# draw subplots\nfor i in range(4):\n    \n    # create empty lists to store training and validation scores\n    training_score_list = []\n    validation_score_list = []\n    \n    for p_features in p_features_list:\n        # create data\n        X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n        # fit the model\n        L = Lasso(alpha = alpha_list[i])\n        L.fit(X_train, y_train)\n\n        # compute and append scores\n        training_score = L.score(X_train, y_train).round(4)\n        validation_score = L.score(X_val, y_val).round(4)\n        training_score_list.append(training_score)\n        validation_score_list.append(validation_score)\n    \n    # label, axis, and plot\n    axarr[axarr_list[i]].set_title(\"alpha = \" + str(alpha_list[i]))\n    axarr[axarr_list[i]].set(xlabel = \"Number of Features\", ylabel = \"Score\")\n    l1 = axarr[axarr_list[i]].axvline(x = n_train, ymin = -0.3, ymax = 1.1, linestyle = 'dashed', label = \"n_train\")\n    l2 = axarr[axarr_list[i]].plot(p_features_list, training_score_list, label = \"training score\")\n    l3 = axarr[axarr_list[i]].plot(p_features_list, validation_score_list, label = \"validation score\")\n    \nlabels = [\"n_train\", \"training score\", \"validation score\"]\nfig.legend([l1, l2, l3], labels=labels, loc=\"upper right\")   \nplt.subplots_adjust(hspace=.9)    \n\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.808e-02, tolerance: 4.225e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.038e-01, tolerance: 5.049e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.498e-01, tolerance: 7.947e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.667e-02, tolerance: 6.139e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.698e-02, tolerance: 6.218e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.221e-02, tolerance: 7.196e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.136e-02, tolerance: 7.467e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.402e-02, tolerance: 6.940e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.031e-01, tolerance: 4.042e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.051e-01, tolerance: 4.241e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.186e-02, tolerance: 4.265e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.031e-01, tolerance: 3.871e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.212e-01, tolerance: 5.579e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.007e-02, tolerance: 5.079e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.599e-01, tolerance: 6.885e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.133e-01, tolerance: 3.400e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.964e-02, tolerance: 5.205e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.317e-01, tolerance: 4.415e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.260e-01, tolerance: 4.767e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.183e-01, tolerance: 6.540e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.009e-01, tolerance: 4.112e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.840e-01, tolerance: 5.782e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.021e-01, tolerance: 5.594e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.103e-01, tolerance: 4.865e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.373e-02, tolerance: 6.901e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.792e-02, tolerance: 6.280e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.163e-01, tolerance: 6.635e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.588e-01, tolerance: 6.772e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.419e-01, tolerance: 5.226e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.684e-01, tolerance: 6.127e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.319e-01, tolerance: 4.500e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.237e-01, tolerance: 6.582e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.361e-01, tolerance: 5.225e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.346e-01, tolerance: 5.854e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.944e-01, tolerance: 7.062e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.271e-01, tolerance: 7.026e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.180e-01, tolerance: 5.210e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.399e-01, tolerance: 7.281e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.521e-01, tolerance: 6.080e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.527e-01, tolerance: 7.766e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.611e-01, tolerance: 7.308e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.520e-01, tolerance: 6.846e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.784e-01, tolerance: 5.883e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.946e-01, tolerance: 5.677e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.806e-01, tolerance: 7.813e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.046e-01, tolerance: 5.827e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.180e-01, tolerance: 7.432e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.051e-01, tolerance: 6.311e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.625e-01, tolerance: 5.379e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.354e-01, tolerance: 6.337e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.906e-01, tolerance: 6.464e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.312e-01, tolerance: 6.317e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.512e-01, tolerance: 6.583e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.037e-01, tolerance: 6.413e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.982e-01, tolerance: 6.332e-02\n  model = cd_fast.enet_coordinate_descent(\n/Applications/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.149e-01, tolerance: 7.122e-02\n  model = cd_fast.enet_coordinate_descent(\n/var/folders/d7/1kbvm4bx4213x2pp7svzg_p40000gp/T/ipykernel_53956/1696307001.py:49: UserWarning: You have mixed positional and keyword arguments, some input may be discarded.\n  fig.legend([l1, l2, l3], labels=labels, loc=\"upper right\")\n\n\n\n\n\nAs alpha becomes smaller, the model tends to reach a better score and reach it more quickly. The score is also significantly higher compared to the linear regression. However, the effect of overfitting remains when the number of features is too large."
  },
  {
    "objectID": "posts/allocative_bias/bias.html",
    "href": "posts/allocative_bias/bias.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "#import\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\n\nSTATE = \"AL\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n  \n    \n      \n      RT\n      SERIALNO\n      DIVISION\n      SPORDER\n      PUMA\n      REGION\n      ST\n      ADJINC\n      PWGTP\n      AGEP\n      ...\n      PWGTP71\n      PWGTP72\n      PWGTP73\n      PWGTP74\n      PWGTP75\n      PWGTP76\n      PWGTP77\n      PWGTP78\n      PWGTP79\n      PWGTP80\n    \n  \n  \n    \n      0\n      P\n      2018GQ0000049\n      6\n      1\n      1600\n      3\n      1\n      1013097\n      75\n      19\n      ...\n      140\n      74\n      73\n      7\n      76\n      75\n      80\n      74\n      7\n      72\n    \n    \n      1\n      P\n      2018GQ0000058\n      6\n      1\n      1900\n      3\n      1\n      1013097\n      75\n      18\n      ...\n      76\n      78\n      7\n      76\n      80\n      78\n      7\n      147\n      150\n      75\n    \n    \n      2\n      P\n      2018GQ0000219\n      6\n      1\n      2000\n      3\n      1\n      1013097\n      118\n      53\n      ...\n      117\n      121\n      123\n      205\n      208\n      218\n      120\n      19\n      123\n      18\n    \n    \n      3\n      P\n      2018GQ0000246\n      6\n      1\n      2400\n      3\n      1\n      1013097\n      43\n      28\n      ...\n      43\n      76\n      79\n      77\n      80\n      44\n      46\n      82\n      81\n      8\n    \n    \n      4\n      P\n      2018GQ0000251\n      6\n      1\n      2701\n      3\n      1\n      1013097\n      16\n      25\n      ...\n      4\n      2\n      29\n      17\n      15\n      28\n      17\n      30\n      15\n      1\n    \n  \n\n5 rows × 286 columns\n\n\n\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      RAC1P\n      ESR\n    \n  \n  \n    \n      0\n      19\n      18.0\n      5\n      17\n      2\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      1\n      6.0\n    \n    \n      1\n      18\n      18.0\n      5\n      17\n      2\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      2\n      6.0\n    \n    \n      2\n      53\n      17.0\n      5\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      2\n      1\n      2\n      2\n      1.0\n      1\n      1\n      6.0\n    \n    \n      3\n      28\n      19.0\n      5\n      16\n      2\n      NaN\n      1\n      1.0\n      2.0\n      1\n      1\n      2\n      2\n      2.0\n      1\n      1\n      6.0\n    \n    \n      4\n      25\n      12.0\n      5\n      16\n      1\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      1.0\n      2\n      1\n      6.0\n    \n  \n\n\n\n\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n\nfor obj in [features, label, group]:\n    print(obj.shape)\n\n(47777, 15)\n(47777,)\n(47777,)"
  },
  {
    "objectID": "posts/linear_reg/linear_reg.html#implement-linear-regression-two-ways",
    "href": "posts/linear_reg/linear_reg.html#implement-linear-regression-two-ways",
    "title": "Implementing Linear Regression",
    "section": "Implement Linear Regression Two Ways",
    "text": "Implement Linear Regression Two Ways\n\n%load_ext autoreload\n%autoreload 2\n\n\n# import packages\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom linear_reg import LinearRegression\nfrom sklearn.linear_model import Lasso\n\n\n# generate dataset\nnp.random.seed(1234)\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\n# plot dataset\n\n# set variables and parameters\nnp.random.seed(1234)\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nAfter obtaining the dataset, we firstly fit the model with the anlytic method and then inspect the training score, the validation score, and the weight.\n\n# analytic method\nnp.random.seed(1234)\n\nLR = LinearRegression()\nLR.fit(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\nLR.w\n\nTraining score = 0.2117\nValidation score = 0.3233\n\n\narray([0.36147991, 0.83389459])\n\n\nThen, we fit the model with the regular gradient and inspect the weight. We can see that the optimized weights of the two methods are quite close.\n\n# regular gradient\nnp.random.seed(1234)\nLR_reg = LinearRegression()\nLR_reg.fit_gradient(X_train, y_train, alpha = 0.001, max_epochs = 1000)\nLR_reg.w\n\narray([0.34744356, 0.84172573])\n\n\nLastly, we implement the stochastic gradient and still get a similar final weight.\n\n# stochastic gradient\nnp.random.seed(1234)\nLR_sto = LinearRegression()\nLR_sto.fit(X_train, y_train, method = 'gradient', alpha = 0.001, m_epochs = 1000)\nLR_sto.w\n\narray([0.35754189, 0.83609168])\n\n\nIn the following graphs, we see how the score changed over time for the regular and the stochastic gradient descent respectively.\n\nplt.plot(LR_reg.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\nplt.title(\"The Change of Score for Regular Gradient Descent\")\n\nText(0.5, 1.0, 'The Change of Score for Regular Gradient Descent')\n\n\n\n\n\n\nplt.plot(LR_sto.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\nplt.title(\"The Change of Score for Stochastic Gradient Descent\")\n\nText(0.5, 1.0, 'The Change of Score for Stochastic Gradient Descent')"
  },
  {
    "objectID": "posts/gebru/gebru.html",
    "href": "posts/gebru/gebru.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "Dr. Timnit Gebru of the Distributed AI Research Institute is virtually visiting our class (CSCI 0451 Machine Learning) and giving a public zoom lecture on bias and social impacts of artificial intelligence. The public talk will take place 7:00–8:15 PM on April 24, 2023 at Hillcrest 103.\nDr. Gebru is a prominent Black computer scientist and researcher, whose work concentrates on algorithmic bias and data mining, and she is also an advocate for ethical AI and diversity in her field. She was honored as one of Fortune’s 50 Greatest Leaders worldwide and recognized as one of ten individuals who significantly influenced science in 2021 by Nature.\nDr. Gebru joined Google in 2018, where she co-led an AI ethics team with Margaret Mitchell. Gebru focused on AI’s societal implications and advocated for responsible technology. In 2019, she called for Amazon to stop selling biased facial recognition technology to law enforcement. In December 2020, her employment with Google ended controversially after disagreements over a research paper on the dangers of large language models. The incident led to widespread criticism and support from Google employees, academics, and civil society. Google’s CEO, Sundar Pichai, later apologized but did not clarify whether Gebru was terminated or resigned. In the wake of the incident, two Google employees resigned, and multiple investigations into the company’s treatment of minority employees were launched. In June 2021, Gebru announced plans to establish an independent research institute, and in December 2021, she launched the Distributed Artificial Intelligence Research Institute (DAIR), focusing on AI’s impact on marginalized communities.\n\n\n\nDr. Gebru’s talk, titled “Fairness, Accountability, Transparency, and Ethics in Computer Vision,” begins by addressing biases and harm caused by computer vision technology. She asserts that white supremacy and colorism manifest differently worldwide, but marginalized groups, often harmed by existing discrimination, lack representation in technology. In a survey investigating people’s opinions on computer vision technology usage and its greatest positive and negative potentials, she demonstrates that the perceived benefits and drawbacks depend on context, as the technology can be advantageous for unsupervised groups but detrimental for targeted ones. Dr. Gebru provides examples of harmful applications, such as software misidentifying terrorists, emotion detection tools like HireVue misinterpreting internal states, and police using facial recognition to target protesters.\nExpanding the discussion on the lack of diversity in datasets, she refers to Mimi Onuoha’s quote, reminding people that both data collectors and subjects are human beings. Dr. Gebru cites examples of biased datasets, leading to higher error rates for black women in facial recognition systems and object classification systems displaying biases against Asian and African cultures.\nDr. Gebru emphasizes that understanding should not be limited to diverse datasets. Visibility is not inclusion, and social and structural problems cannot be ignored. She advocates for considering data acquisition methods, its applications, and potential targeting of marginalized groups such as black and brown people, protesters, and transgender communities.\nAddressing structural representation in the computer vision field, she highlights that current ethics boards often do not adequately represent marginalized groups. Dr. Gebru argues that fairness transcends datasets and mathematics, encompassing societal issues, and urges us to consider how technologies are used to marginalize certain groups. To move towards socially responsible and ethics-informed research practices, she reminds researchers that technology is not value-neutral, and they are accountable for the intended and unintended consequences of their work. Researchers should consider multiple stakeholders and be attentive to the social relations and power dynamics shaping the construction and use of technology.\nIn conclusion, Dr. Gebru’s talk underscores the importance of ethical considerations in computer vision technologies, highlighting potential biases and the need for transparency, accountability, and diversity in AI systems development.\n\n\n\nSince both data collectors and subjects are human beings, it is crucial to be aware of how technology is used to target vulnerable groups.\n\n\n\n\nWhat are some practical ways for AI researchers and practitioners to stay informed about and incorporate ethical considerations into their work consistently?\nIn your opinion, what are the most effective strategies for fostering interdisciplinary collaboration between AI researchers, social scientists, and policymakers to address the ethical implications of AI technologies?"
  },
  {
    "objectID": "reflection/mid-course.html",
    "href": "reflection/mid-course.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Yanruo (Alice) Zhang\n\n\nIn this section I’ll ask you to fill in some data. You don’t have to give precise numbers – approximate, conversational responses are fine. For example, when I ask “how often have you attended class,” good answers include “almost always,” “I’ve missed three times,” “about 75% of the time,” “not as often as I want,” etc.\n\n\n\nHow often have you attended class? (e.g. “almost always,” “I missed three times,” etc.) every class\nHow often have you taken notes on the core readings ahead of the class period? I have taken notes for about three readings\nHow often have you been prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? I am prepared every time\nHow many times have you actually presented the daily warm-up to your team? whenever it was my turn or when everybody presented\nHow many times have you asked your team for help while presenting the daily warm-up? for about 25% of the times I will propose some questions in mind to the whole group\nHow often have you learned something new from a teammate’s presentation of the daily warm-up? about 75% of the time (more concise ways of coding, new ideas, etc.)\nHow often have you helped a teammate during the daily warm-up presentation? every one in my group did their job well, and I only participated in discussions when needed\n\n\n\n\n\nHow often have you attended Student Hours or Peer Help? about once every week\nHow often have you asked for or received help from your fellow students? about once or twice for every blog post whenever I am stuck\nHave you been regularly participating in a study group outside class? yes, at least once every blog post\nHow often have you posted questions or answers in Slack? never…\n\n\n\n\n\nHow many blog posts have you submitted? 4\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nNo revisions suggested: 2\nRevisions useful: \nRevisions encouraged: \nIncomplete: \n\nRoughly how many hours per week have you spent on this course outside of class? 8\n\n\n\n\n\nAt the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what have you done in order to pursue your interest?\nI chose to focus on Implementation and Experimentation. For Implementation, I learned many python toolkits to visualize, organize, and process data. Although I could not memorize them, I have practiced how to use them and know how to find them when needed. I also learned effective coding from my peers during warm-up discussions. However, for review purposes, I think maybe it is useful to organize a chart to jog down what kind of packages are used for what. For Experimentation, I focused on assessing the performance of various algorithms in blog post assignments. For example, I chose the Penguin blog post over the theoretical one and did optional work such as fitting with multiple algorithms and testing their accuracy. In terms of communicating the results, I explored data visualization that is not mentioned in our lectures, such as the pair plot to investigate the correlations between pairs of variables.\n\n\n\nFor each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways are you on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what you are doing in order to meet it.\nIn what ways are you not on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what gap you see between where you are and your goal.\nIf there’s any context you want to share about how you are faring relative to your goals, please do!\n\n\n\nThese were my goals: - Submit most blog posts (missing 1 or 2 due to special circumstances is bearable). - Start early for every blog post (within 2 days after the posted day). - Submit the first draft of no more than 3 blog posts after the “best-by” date. - Revise at least half of the blog posts to the “No Revisions Suggested” level. - Propose and complete an additional blog post on an algorithm related to deep learning in bioinformatics, preferably psychiatric genetics.\nFor most of the goals, although falling a bit behind time-wise, I consider myself on track. - For the goal of submitting most blog posts, I have submitted 4 out of 6 so far, and I plan to catch up for at least 1 more this week and next week. - For the goal of starting early (within 2 days after the posted day), I did manage to stay on track for most of the posts except the one posted on Mar 15 (Limits of quantitative methods/Auditing allocative bias). - For the goal of submitting the first draft of no more than 3 blog posts after the “best-by” date, I have submitted 3 blog posts before the “best-by” date, meaning that I have used 1/3 of my quota before reaching the middle point. However, I do not think I could finish the Mar 15 one on time. - The blog I had feedback on requires no revision, so I am on track to meet the goal of revising at least half of the blog posts to the “No Revisions Suggested” level.\nThe one goal that I don’t think I could be on track is proposing and completing an additional blog post on an algorithm related to deep learning in bioinformatics, preferably psychiatric genetics. Due to huge size of genomic data and a lack of understanding to the domain knowledge (hundreds or thousands of individual genomic data would be around 500 GB; not sure how we could do anything interesting with summary statistics), I also decided not to do my project on psychiatric genetics. It is unfortunate that I could not use this class to explore my interest in this field in terms of the blog post and the project. To compensate, I might invest some time in researching and implementing related algorithms, but this might not be a priority and only a plus due to the workload I have.\n\n\n\nI am completely on track for the following three goals: - Complete all core readings, take reading notes when needed, and read lecture notes prior to each class. - Attend study group to work with at least one other student at least once for each assignment. - Spend no longer than 2 hours on each warm-up.\nI am falling behind on these two goals: - For reviewing self-taken notes, lecture notes, and important sections of the readings within a day after each class, I have done 50% of the review for the lectures and not the rest. Writing this reflection makes me realize how much I value timely review, and thus will try to catch up for the remaining weeks. - For posting at least one question, and trying to answer at least one question in Slack, I have not posted or answered any questions so far because most of my questions are solved in TA hours and classmate discussions.\n\n\n\nI have formed a group and plan to stick to all the deadlines, so I think so far, I am staying on track.\n\n\n\nIs there anything else that you want to share with me about what you have learned, how you have participated, or what you have achieved in CSCI 0451?\nI personally will benifit from a slightly faster pace of the lectures.\n\n\n\n\nTake 15 minutes to look back on your responses in each of the sections above. Then, state the letter grade that you feel reflects your learning, participation, and achievement in CSCI 0451 so far. Here are some soundbytes to help guide your thinking:\n\n\nAn A sounds like:\n\n“I am very proud of my time in this course.”\n“I have grown significantly in multiple ways that matter to me.”\n“I am ready to take the theory, techniques, and ideas of this course into my future classes, projects, hobbies, or career.”\n\nA B sounds like:\n\n“I had some opportunities to learn more, overall I feel good about my time in this course.”\n“I am able to explain some new things or achieve new tasks.”\n“I can see a few ideas from this course that will be relevant for my future classes, projects, hobbies, or career.”\n\nA C sounds like:\n\n“I often made a good effort, but I missed many opportunities to get more out of my time in this course.”\n“I might be able to complete some new tasks related to the course content, but only with significant further guidance.”\n“I don’t see any ways to take the contents of this course into my future classes, projects, hobbies, or career.”\n\nYou might find that some of these soundbytes resonate and other’s don’t! Take some time, see what feels right, and don’t be afraid to celebrate your achievements.\n\nUpon reflection, I feel that my learning, participation, and achievement in CSCI 0451 (so far) are best reflected by a grade of A-\n\n\n\n\n\nYou may feel disappointed by your reflection. Sometimes we don’t achieve all our goals – it happens and it’s normal! If you are feeling disappointed by how you’ve learned, participated, or achieved in CSCI 0451, then feel free to write something about that below. Feel free to just write your feelings. If you have ideas for how to move forward, include those too! We’ll talk.\nI am a bit disappointed by my reflection. I feel like the material is not that difficult, and that I should have achieved more. Because of my procrastination and seasonal depression, I did not push myself to do more, and that disappoints me. I think setting time aside to simply start working on something and ask for peer supervision might be helpful. Also, I might benefit from going to OH to talk about how to explore ML in bioinformatics in order to make myself feel that I am learning something applicable in another field I love."
  },
  {
    "objectID": "posts/final_project/project-blogpost.html",
    "href": "posts/final_project/project-blogpost.html",
    "title": "Speech Emotion Recognition",
    "section": "",
    "text": "References\n\nBadshah, Abdul Malik, Jamil Ahmad, Nasir Rahim, and Sung Wook Baik. 2017. “Speech Emotion Recognition from Spectrograms with Deep Convolutional Neural Network.” In 2017 International Conference on Platform Technology and Service (PlatCon), 1–5. IEEE.\n\n\nHasan, Md Rakibul, Md Mahbub Hasan, and Md Zakir Hossain. 2021. “How Many Mel-Frequency Cepstral Coefficients to Be Utilized in Speech Recognition? A Study with the Bengali Language.” The Journal of Engineering 2021 (12): 817–27.\n\n\nKumbhar, Harshawardhan S, and Sheetal U Bhandari. 2019. “Speech Emotion Recognition Using MFCC Features and LSTM Network.” In 2019 5th International Conference on Computing, Communication, Control and Automation (ICCUBEA), 1–3. IEEE.\n\n\nLivingstone, Steven R, and Frank A Russo. 2018. “The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A Dynamic, Multimodal Set of Facial and Vocal Expressions in North American English.” PloS One 13 (5): e0196391.\n\n\nXu, Mingke, Fan Zhang, and Samee U Khan. 2020. “Improve Accuracy of Speech Emotion Recognition with Attention Head Fusion.” In 2020 10th Annual Computing and Communication Workshop and Conference (CCWC), 1058–64. IEEE."
  }
]