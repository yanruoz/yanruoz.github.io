[
  {
    "objectID": "posts/perceptron/perceptron_blog.html",
    "href": "posts/perceptron/perceptron_blog.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "source code link: https://github.com/yanruoz/yanruoz.github.io/blob/main/posts/perceptron/perceptron.py"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#implementation-of-the-perceptron-update",
    "href": "posts/perceptron/perceptron_blog.html#implementation-of-the-perceptron-update",
    "title": "Perceptron Blog",
    "section": "Implementation of the Perceptron Update",
    "text": "Implementation of the Perceptron Update\nIn the fit() function, I first reset history as an empty array to avoid accumulating histories of multiple datasets. Then, I unpack the tuple returned by X.shape by assigning the number of data points to n, and the number of features to p. Then, I initialize a random initial weight vector w_tilt(0) as a 1D array composed of floats \\(\\in [1, -1]\\). Next, I modify the input X into X_ (which contains a column of 1s and corresponds to X_tilde).\nWithin the for loop that iterates within the range of the max steps input by the user, I generate a random number within the amount of data points (n). We have the equation for the perceptron update: \\(\\tilde{w}^{(t+1)}=\\tilde{w}^{(t)}+ \\mathbb{1}{(\\tilde{y_i} \\langle \\tilde{w}^{(t)}, \\tilde{x_i} \\rangle <0)}\\tilde{y_i} \\tilde{x_i}\\), where \\(\\tilde{y_i} = 2y_i-1\\) to make \\(\\tilde{y_i}\\) take the value of (-1 and 1) instead of (0 and 1). To implement it, I calculate different parts of the equation respectively and update the weight by multiplying them:\n---\ny_tilde_i = 2 * y[i] - 1  \nx_tilde_i = X_[i]\ny_tilde_pred = int(y_tilde_i * np.dot(x_tilde_i, self.w) < 0)\nself.w += y_tilde_pred * y_tilde_i * x_tilde_i\n---\n\n%load_ext autoreload\n%autoreload 2\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport random\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_moons"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#experiment",
    "href": "posts/perceptron/perceptron_blog.html#experiment",
    "title": "Perceptron Blog",
    "section": "Experiment",
    "text": "Experiment\n\nExperiment 1: Linearly-separable 2d data\nUsing a 2d dataset in which the data is linearly separable, the perceptron algorithm converges to weight vector \\(\\tilde{w}\\), which is the separating line shown in Fig 1.1.\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = n, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nplt.title(\"Fig 1.1: Linearly separable 2d data points\")\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThe evolution of the accuracy over training is shown in the printed history and Fig 1.2; the final accuracy of 1 confirms that the perceptron algorithm converges.\n\nnp.random.seed(12345)\np = Perceptron()\np.fit(X, y, max_steps = 10000)\np.w\nprint(p.history[-10:])\nfig = plt.plot(p.history)\nplt.title(\"Fig 1.2: Evolution of the accuracy over training of linearly separable 2d data\")\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\n\n\n\n\nnp.random.seed(12345)\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nplt.title(\"Fig 1.3: Linearly separable 2d data points with the separating line\")\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\np.score(X, y)\n\n1.0\n\n\n\n\n\n\n\nExperiment 2: Non-linearly-separable 2d data\nIn the case of not linearly separable 2d data, we generate such a dataset using make_moons function. As shown in the Fig 2.1, the data overlaps in a way that is not linearly separable.\n\nnp.random.seed(123)\n# Generate a non-linearly separable dataset with 100 samples\nX, y = make_moons(n_samples=100, noise=0.1, random_state=42)\n\n# Plot the dataset\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\nplt.title(\"Fig 2.1: Non-linearly-separable 2d data points\")\nplt.show()\n\n\n\n\nThe evolution of accuracy over training also shows that the algorithm does not converge when the iteration of max steps is completed.\n\nnp.random.seed(123)\np = Perceptron()\np.fit(X, y, max_steps = 1000)\np.w\nprint(p.history[-10:])\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n[0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86]\n\n\n\n\n\nThe separating line in the final iteration is shown in Fig.2.3, which does not separate the data obviously.\n\nnp.random.seed(123)\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nplt.title(\"Fig 2.3: Non-linearly-separable 2d data points with the separating line\")\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\np.score(X, y)\n\n0.86\n\n\n\n\n\n\n\nExperiment 3: 5d data\nThe perceptron algorithm is also able to work in more than 2 dimensions. In this experiment, the dataset has 5 features. The evolution of the accuracy over training is shown in Fig 3. Since the final score is less than 1, the algorithm does not converge; therefore, the data is not linearly separable.\n\nnp.random.seed(230230)\n\nn = 100\np_features = 5\n\nX, y = make_blobs(n_samples = n, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\np.w\nprint(p.history[-10:])\nfig = plt.plot(p.history)\nplt.title(\"Fig 1.2: Evolution of the accuracy over training of 5d data\")\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n[0.99, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98]"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#thinking-question",
    "href": "posts/perceptron/perceptron_blog.html#thinking-question",
    "title": "Perceptron Blog",
    "section": "Thinking Question",
    "text": "Thinking Question\nQ: What is the runtime complexity of a single iteration of the perceptron algorithm update as described by Equation 1? Assume that the relevant operations are addition and multiplication. Does the runtime complexity depend on the number of data points ? What about the number of features?\nR: The runtime complexity of a single iteration is \\(O(N)\\) because when calculating the dot product between the weight and the feature vector, we only consider one data point at a time. Therefore, the runtime complexity only depends on the number of features and not on the number of data points in a single iteration."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Implementation of perceptron algorithm.\n\n\n\n\n\n\nFeb 22, 2023\n\n\nAlice (Yanruo) Zhang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/logistic_regression/gradient_blog.html",
    "href": "posts/logistic_regression/gradient_blog.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport random\n\nfrom gradient import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}\n\n\n\nnp.random.seed(300)\n# make the data\nX, y = make_blobs(n_samples = 200, n_features = 2, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n# fit the model\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.01, max_epochs = 1000)\n\n# inspect the fitted value of w\nLR.w \n\n# check history\nprint(f\"{LR.score_history[-10:] = }\")\nprint(f\"{LR.loss_history[-10:] = }\")\n\nLR.score_history[-10:] = [0.945, 0.945, 0.945, 0.945, 0.945, 0.945, 0.945, 0.945, 0.945, 0.945]\nLR.loss_history[-10:] = [0.21195696250763846, 0.21190646535032115, 0.2118560735821291, 0.2118057868924457, 0.21175560497185045, 0.21170552751211297, 0.2116555542061873, 0.21160568474820646, 0.21155591883347674, 0.21150625615847218]\n\n\n\nnp.random.seed(666)\n# stochasitc gradient (with momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = .05) \n\n# check history\nprint(f\"{LR.loss_history[-10:] = }\")\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\n# stochasitc gradient (no momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .05)\n# check history\nprint(f\"{LR.loss_history[-10:] = }\")\n\n# plot\nnum_steps = len(LR.loss_history)\nprint(num_steps)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n# gradient\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 100)\n\n# check history\nprint(f\"{LR.loss_history[-10:] = }\")\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\nprint(f\"{num_steps = }\")\n\n# plot loglog\nplt.loglog()\n\nlegend = plt.legend()\n\nLR.loss_history[-10:] = [0.1749583312858246, 0.1749519561793999, 0.1749437099635265, 0.17493553065714354, 0.17492769890213414, 0.17492149041408503, 0.17491763977785968, 0.17491133797270816, 0.1749047354349967, 0.17489868126848518]\nLR.loss_history[-10:] = [0.1749198792959875, 0.17491325642416886, 0.17490916154008823, 0.17490385425048274, 0.17489673431520103, 0.17489073792094043, 0.17488394501308274, 0.17487824708887026, 0.1748739817742154, 0.17486973253491353]\n100\nLR.loss_history[-10:] = [0.25505411669552247, 0.2541391227604576, 0.2532409198374154, 0.25235906739303954, 0.2514931400488848, 0.2506427269360351, 0.2498074310824243, 0.24898686883094479, 0.24818066928655402, 0.24738847379070777]\nnum_steps = 100\n\n\n\n\n\n\nExperiments\nAfter you have tested and implemented your class, please perform experiments in which you show examples of the following phenomena:\nA case in which gradient descent does not converge to a minimizer because the learning rate is too large.\nA case in which the choice of batch size influences how quickly the algorithm converges. If you implemented momentum, a case in which the use of momentum significantly speeds up convergence. In at least one of these experiments, generate some synthetic data (it’s fine to use make_blobs) for data of at least 10 feature dimensions.\n\nA case in which gradient descent does not converge to a minimizer because the learning rate is too large.\n\n\nnp.random.seed(306)\n#gradient\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 100, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\nprint(f\"{num_steps = }\")\n\n# plot loglog\nplt.loglog()\n\nlegend = plt.legend()\n\nnum_steps = 100\n\n\n\n\n\n\nA case in which the choice of batch size influences how quickly the algorithm converges.\n\n\nnp.random.seed(307)\n# stochasitc gradient (no momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 5, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (batch size = 5)\")\n\n# stochasitc gradient (no momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\n#print(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (batch size = 10)\")\n\n# plot loglog\nplt.loglog()\n\nlegend = plt.legend()\n\n64\n100\n\n\n\n\n\n\nA case in which the use of momentum significantly speeds up convergence.\n\n\nnp.random.seed(312)\n# make the data (10 features)\nX, y = make_blobs(n_samples = 200, n_features = 2, centers = [(-1, -1), (1, 1)])\n\n# stochasitc gradient (with momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X2, y2, \n                  m_epochs = 100, \n                  momentum = True, \n                  batch_size = 4, \n                  alpha = .07) \n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\n# print(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\n# stochasitc gradient (no momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X2, y2, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 4, \n                  alpha = .07)\n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\n# print(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n# plot loglog\nplt.loglog()\n\nlegend = plt.legend()\n\n27\n36"
  }
]