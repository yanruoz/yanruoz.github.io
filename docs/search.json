[
  {
    "objectID": "posts/perceptron/perceptron_blog.html",
    "href": "posts/perceptron/perceptron_blog.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "source code link: https://github.com/yanruoz/yanruoz.github.io/blob/main/posts/perceptron/perceptron.py"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#implementation-of-the-perceptron-update",
    "href": "posts/perceptron/perceptron_blog.html#implementation-of-the-perceptron-update",
    "title": "Perceptron Blog",
    "section": "Implementation of the Perceptron Update",
    "text": "Implementation of the Perceptron Update\nIn the fit() function, I first reset history as an empty array to avoid accumulating histories of multiple datasets. Then, I unpack the tuple returned by X.shape by assigning the number of data points to n, and the number of features to p. Then, I initialize a random initial weight vector w_tilt(0) as a 1D array composed of floats \\(\\in [1, -1]\\). Next, I modify the input X into X_ (which contains a column of 1s and corresponds to X_tilde).\nWithin the for loop that iterates within the range of the max steps input by the user, I generate a random number within the amount of data points (n). We have the equation for the perceptron update: \\(\\tilde{w}^{(t+1)}=\\tilde{w}^{(t)}+ \\mathbb{1}{(\\tilde{y_i} \\langle \\tilde{w}^{(t)}, \\tilde{x_i} \\rangle <0)}\\tilde{y_i} \\tilde{x_i}\\), where \\(\\tilde{y_i} = 2y_i-1\\) to make \\(\\tilde{y_i}\\) take the value of (-1 and 1) instead of (0 and 1). To implement it, I calculate different parts of the equation respectively and update the weight by multiplying them:\n---\ny_tilde_i = 2 * y[i] - 1  \nx_tilde_i = X_[i]\ny_tilde_pred = int(y_tilde_i * np.dot(x_tilde_i, self.w) < 0)\nself.w += y_tilde_pred * y_tilde_i * x_tilde_i\n---\n\n%load_ext autoreload\n%autoreload 2\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport random\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_moons"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#experiment",
    "href": "posts/perceptron/perceptron_blog.html#experiment",
    "title": "Perceptron Blog",
    "section": "Experiment",
    "text": "Experiment\n\nExperiment 1: Linearly-separable 2d data\nUsing a 2d dataset in which the data is linearly separable, the perceptron algorithm converges to weight vector \\(\\tilde{w}\\), which is the separating line shown in Fig 1.1.\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = n, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nplt.title(\"Fig 1.1: Linearly separable 2d data points\")\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThe evolution of the accuracy over training is shown in the printed history and Fig 1.2; the final accuracy of 1 confirms that the perceptron algorithm converges.\n\nnp.random.seed(12345)\np = Perceptron()\np.fit(X, y, max_steps = 10000)\np.w\nprint(p.history[-10:])\nfig = plt.plot(p.history)\nplt.title(\"Fig 1.2: Evolution of the accuracy over training of linearly separable 2d data\")\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\n\n\n\n\nnp.random.seed(12345)\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nplt.title(\"Fig 1.3: Linearly separable 2d data points with the separating line\")\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\np.score(X, y)\n\n1.0\n\n\n\n\n\n\n\nExperiment 2: Non-linearly-separable 2d data\nIn the case of not linearly separable 2d data, we generate such a dataset using make_moons function. As shown in the Fig 2.1, the data overlaps in a way that is not linearly separable.\n\nnp.random.seed(123)\n# Generate a non-linearly separable dataset with 100 samples\nX, y = make_moons(n_samples=100, noise=0.1, random_state=42)\n\n# Plot the dataset\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\nplt.title(\"Fig 2.1: Non-linearly-separable 2d data points\")\nplt.show()\n\n\n\n\nThe evolution of accuracy over training also shows that the algorithm does not converge when the iteration of max steps is completed.\n\nnp.random.seed(123)\np = Perceptron()\np.fit(X, y, max_steps = 1000)\np.w\nprint(p.history[-10:])\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n[0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86]\n\n\n\n\n\nThe separating line in the final iteration is shown in Fig.2.3, which does not separate the data obviously.\n\nnp.random.seed(123)\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nplt.title(\"Fig 2.3: Non-linearly-separable 2d data points with the separating line\")\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\np.score(X, y)\n\n0.86\n\n\n\n\n\n\n\nExperiment 3: 5d data\nThe perceptron algorithm is also able to work in more than 2 dimensions. In this experiment, the dataset has 5 features. The evolution of the accuracy over training is shown in Fig 3. Since the final score is less than 1, the algorithm does not converge; therefore, the data is not linearly separable.\n\nnp.random.seed(230230)\n\nn = 100\np_features = 5\n\nX, y = make_blobs(n_samples = n, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\np.w\nprint(p.history[-10:])\nfig = plt.plot(p.history)\nplt.title(\"Fig 1.2: Evolution of the accuracy over training of 5d data\")\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n[0.99, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98]"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#thinking-question",
    "href": "posts/perceptron/perceptron_blog.html#thinking-question",
    "title": "Perceptron Blog",
    "section": "Thinking Question",
    "text": "Thinking Question\nQ: What is the runtime complexity of a single iteration of the perceptron algorithm update as described by Equation 1? Assume that the relevant operations are addition and multiplication. Does the runtime complexity depend on the number of data points ? What about the number of features?\nR: The runtime complexity of a single iteration is \\(O(N)\\) because when calculating the dot product between the weight and the feature vector, we only consider one data point at a time. Therefore, the runtime complexity only depends on the number of features and not on the number of data points in a single iteration."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Regular and stochastic logistic regression fit models with experiements on learning rate, batch size, momentum, and feature numbers.\n\n\n\n\n\n\nMar 6, 2023\n\n\nAlice (Yanruo) Zhang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplementation of perceptron algorithm.\n\n\n\n\n\n\nFeb 22, 2023\n\n\nAlice (Yanruo) Zhang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/logistic_regression/gradient_blog.html",
    "href": "posts/logistic_regression/gradient_blog.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "Algorithm Implementation\nFirst, I implement the regular gradient descent model to calculate the loss function. In the fit() function, I first reset the history and obtain the data points and the number of features from the given dataset. Then, I initialized a random initial weight vector. In the main loop, the gradient is calculated for computing the loss and updating the weight. When the losses become indistinguishable, we consider finding the minimalized loss and thus terminate the loop.\nBased on the regular gradient descent model, we implement one of its key variants, the stochastic gradient descent with an optional momentum feature. In the fit_stochasitic() function, I compute the stochastic gradient by picking a random subset and computing the corresponding gradient. Specifically, we use a nested for loop. The outer loop iterates through the max number of epochs input by the user by shuffling them first. The inner loop iterates through arrays of equal size indicated by the user. Each array is a batch. Within each batch, we calculate the stochastic gradient and conduct the update.\n\n\nPerformance Check\nInitiate autoreload and import packages.\n\n%load_ext autoreload\n%autoreload 2\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport random\n\nfrom gradient import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}\n\n\nGenerate a 2D dataset.\n\nnp.random.seed(300)\n# make the data\nX, y = make_blobs(n_samples = 200, n_features = 2, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nFit the model for regular gradient and check history.\n\n# fit the model\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.01, max_epochs = 1000)\n\n# inspect the fitted value of w\nLR.w \n\n# check history\nprint(f\"{LR.score_history[-10:] = }\")\nprint(f\"{LR.loss_history[-10:] = }\")\n\nLR.score_history[-10:] = [0.93, 0.93, 0.93, 0.93, 0.93, 0.93, 0.93, 0.93, 0.93, 0.93]\nLR.loss_history[-10:] = [0.23377412203767803, 0.2337256570612906, 0.2336772939128487, 0.2336290322835982, 0.2335808718660169, 0.2335328123538083, 0.2334848534418953, 0.23343699482641447, 0.23338923620470986, 0.2333415772753273]\n\n\nSeems right. Now we fit the models for stochasitc gradient (with momentum), stochasitc gradient (no momentum), and regular gradient. Then, we plot all three on the loglog plot to see how they converge over iterations.\n\nnp.random.seed(666)\n# stochasitc gradient (with momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = .05) \n\n# check history\nprint(f\"{LR.loss_history[-10:] = }\")\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\n# stochasitc gradient (no momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .05)\n# check history\nprint(f\"{LR.loss_history[-10:] = }\")\n\n# plot\nnum_steps = len(LR.loss_history)\nprint(num_steps)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n# gradient\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 100)\n\n# check history\nprint(f\"{LR.loss_history[-10:] = }\")\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\nprint(f\"{num_steps = }\")\n\n# plot loglog\nplt.loglog()\n\nlegend = plt.legend()\n\nLR.loss_history[-10:] = [0.19729864650198484, 0.19728844411058305, 0.19727935468954463, 0.19727046593738962, 0.19725968767836846, 0.19725286746340295, 0.19724485117413124, 0.19723713411086885, 0.1972312121585486, 0.19722350545933254]\nLR.loss_history[-10:] = [0.19732937184575206, 0.19731855298247097, 0.1973095909859817, 0.1973046706827699, 0.19729220743287826, 0.19728488395894772, 0.1972763999564443, 0.19726826759154018, 0.19726040304512601, 0.19725317650122887]\n100\nLR.loss_history[-10:] = [0.2790618158178214, 0.278179397253642, 0.2773132078315364, 0.2764628136516567, 0.27562779596145764, 0.27480775050780504, 0.2740022869216691, 0.2732110281335338, 0.2724336098177659, 0.271669679864307]\nnum_steps = 100\n\n\n\n\n\nAll three models converge given small enough learning rate. In general, the regular gradient model converges slower than the stochastic gradient models and might need more epochs to find a good solution.\n\n\nExperiments\nIt’s time to play with the parameters even more and see how the learning rate, batch size, momentum, and feature numbers!\n\nA case in which gradient descent does not converge to a minimizer because the learning rate is too large.\n\nWhen the learning rate (step size) is too large, as shown below, the model does not converge any more but oscillates instead.\n\nnp.random.seed(306)\n#gradient\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 100, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\nprint(f\"{num_steps = }\")\n\n# plot loglog\nplt.loglog()\n\nlegend = plt.legend()\n\nnum_steps = 100\n\n\n\n\n\n\nA case in which the choice of batch size influences how quickly the algorithm converges.\n\nThe smaller the batch, the more quickly the algorithm converges.\n\nnp.random.seed(307)\n# stochasitc gradient (no momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 5, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (batch size = 5)\")\n\n# stochasitc gradient (no momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\n#print(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (batch size = 10)\")\n\n# plot loglog\nplt.loglog()\n\nlegend = plt.legend()\n\n64\n100\n\n\n\n\n\n\nA case in which the use of momentum significantly speeds up convergence.\n\nI tried multiple random seeds, while sometimes the stochastic gradient models with and without momentum display similar converging rate, the one with momentum sometimes out perform the one without. Therefore, in general, we can conclude that the use of momentum significantly speeds up convergence.\n\nnp.random.seed(312)\n# make the data (10 features)\nX, y = make_blobs(n_samples = 200, n_features = 2, centers = [(-1, -1), (1, 1)])\n\n# stochasitc gradient (with momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X2, y2, \n                  m_epochs = 100, \n                  momentum = True, \n                  batch_size = 4, \n                  alpha = .07) \n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\n# print(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\n# stochasitc gradient (no momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X2, y2, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 4, \n                  alpha = .07)\n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\n# print(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n# plot loglog\nplt.loglog()\n\nlegend = plt.legend()\n\n27\n36"
  }
]