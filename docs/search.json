[
  {
    "objectID": "posts/perceptron/perceptron_blog.html",
    "href": "posts/perceptron/perceptron_blog.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "source code link: https://github.com/yanruoz/yanruoz.github.io/blob/main/posts/perceptron/perceptron.py"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#implementation-of-the-perceptron-update",
    "href": "posts/perceptron/perceptron_blog.html#implementation-of-the-perceptron-update",
    "title": "Perceptron Blog",
    "section": "Implementation of the Perceptron Update",
    "text": "Implementation of the Perceptron Update\nIn the fit() function, I first reset history as an empty array to avoid accumulating histories of multiple datasets. Then, I unpack the tuple returned by X.shape by assigning the number of data points to n, and the number of features to p. Then, I initialize a random initial weight vector w_tilt(0) as a 1D array composed of floats \\(\\in [1, -1]\\). Next, I modify the input X into X_ (which contains a column of 1s and corresponds to X_tilde).\nWithin the for loop that iterates within the range of the max steps input by the user, I generate a random number within the amount of data points (n). We have the equation for the perceptron update: \\(\\tilde{w}^{(t+1)}=\\tilde{w}^{(t)}+ \\mathbb{1}{(\\tilde{y_i} \\langle \\tilde{w}^{(t)}, \\tilde{x_i} \\rangle <0)}\\tilde{y_i} \\tilde{x_i}\\), where \\(\\tilde{y_i} = 2y_i-1\\) to make \\(\\tilde{y_i}\\) take the value of (-1 and 1) instead of (0 and 1). To implement it, I calculate different parts of the equation respectively and update the weight by multiplying them:\n---\ny_tilde_i = 2 * y[i] - 1  \nx_tilde_i = X_[i]\ny_tilde_pred = int(y_tilde_i * np.dot(x_tilde_i, self.w) < 0)\nself.w += y_tilde_pred * y_tilde_i * x_tilde_i\n---\n\n%load_ext autoreload\n%autoreload 2\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport random\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_moons"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#experiment",
    "href": "posts/perceptron/perceptron_blog.html#experiment",
    "title": "Perceptron Blog",
    "section": "Experiment",
    "text": "Experiment\n\nExperiment 1: Linearly-separable 2d data\nUsing a 2d dataset in which the data is linearly separable, the perceptron algorithm converges to weight vector \\(\\tilde{w}\\), which is the separating line shown in Fig 1.1.\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = n, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nplt.title(\"Fig 1.1: Linearly separable 2d data points\")\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThe evolution of the accuracy over training is shown in the printed history and Fig 1.2; the final accuracy of 1 confirms that the perceptron algorithm converges.\n\nnp.random.seed(12345)\np = Perceptron()\np.fit(X, y, max_steps = 10000)\np.w\nprint(p.history[-10:])\nfig = plt.plot(p.history)\nplt.title(\"Fig 1.2: Evolution of the accuracy over training of linearly separable 2d data\")\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\n\n\n\n\nnp.random.seed(12345)\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nplt.title(\"Fig 1.3: Linearly separable 2d data points with the separating line\")\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\np.score(X, y)\n\n1.0\n\n\n\n\n\n\n\nExperiment 2: Non-linearly-separable 2d data\nIn the case of not linearly separable 2d data, we generate such a dataset using make_moons function. As shown in the Fig 2.1, the data overlaps in a way that is not linearly separable.\n\nnp.random.seed(123)\n# Generate a non-linearly separable dataset with 100 samples\nX, y = make_moons(n_samples=100, noise=0.1, random_state=42)\n\n# Plot the dataset\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\nplt.title(\"Fig 2.1: Non-linearly-separable 2d data points\")\nplt.show()\n\n\n\n\nThe evolution of accuracy over training also shows that the algorithm does not converge when the iteration of max steps is completed.\n\nnp.random.seed(123)\np = Perceptron()\np.fit(X, y, max_steps = 1000)\np.w\nprint(p.history[-10:])\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n[0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86]\n\n\n\n\n\nThe separating line in the final iteration is shown in Fig.2.3, which does not separate the data obviously.\n\nnp.random.seed(123)\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nplt.title(\"Fig 2.3: Non-linearly-separable 2d data points with the separating line\")\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\np.score(X, y)\n\n0.86\n\n\n\n\n\n\n\nExperiment 3: 5d data\nThe perceptron algorithm is also able to work in more than 2 dimensions. In this experiment, the dataset has 5 features. The evolution of the accuracy over training is shown in Fig 3. Since the final score is less than 1, the algorithm does not converge; therefore, the data is not linearly separable.\n\nnp.random.seed(230230)\n\nn = 100\np_features = 5\n\nX, y = make_blobs(n_samples = n, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\np.w\nprint(p.history[-10:])\nfig = plt.plot(p.history)\nplt.title(\"Fig 1.2: Evolution of the accuracy over training of 5d data\")\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n[0.99, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98]"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#thinking-question",
    "href": "posts/perceptron/perceptron_blog.html#thinking-question",
    "title": "Perceptron Blog",
    "section": "Thinking Question",
    "text": "Thinking Question\nQ: What is the runtime complexity of a single iteration of the perceptron algorithm update as described by Equation 1? Assume that the relevant operations are addition and multiplication. Does the runtime complexity depend on the number of data points ? What about the number of features?\nR: The runtime complexity of a single iteration is \\(O(N)\\) because when calculating the dot product between the weight and the feature vector, we only consider one data point at a time. Therefore, the runtime complexity only depends on the number of features and not on the number of data points in a single iteration."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "The first part of the blog shows the implementation of least-squares linear regression in two ways: 1) the analytical way of using a formula for the optimal weight vector and 2) the way of using gradient descent. The second part performs experiments to illustrate the effect of overfitting. The last part demonstrates the LASSO regularization, an alternative algorithm that uses a modified loss function with a regularization term, for overparameterized problems.\n\n\n\n\n\n\nApr 3, 2023\n\n\nAlice (Yanruo) Zhang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis blog shows the standard workflow of machine learning through the classification of three penguine species. The primary goal is to determine the smallest number of measurements necessary to confidently determine the species.\n\n\n\n\n\n\nMar 9, 2023\n\n\nAlice (Yanruo) Zhang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nRegular and stochastic logistic regression fit models with experiements on learning rate, batch size, momentum, and feature numbers.\n\n\n\n\n\n\nMar 6, 2023\n\n\nAlice (Yanruo) Zhang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplementation of perceptron algorithm.\n\n\n\n\n\n\nFeb 22, 2023\n\n\nAlice (Yanruo) Zhang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/logistic_regression/gradient_blog.html",
    "href": "posts/logistic_regression/gradient_blog.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "Algorithm Implementation\nFirst, I implement the regular gradient descent model to calculate the loss function. In the fit() function, I first reset the history and obtain the data points and the number of features from the given dataset. Then, I initialized a random initial weight vector. In the main loop, the gradient is calculated for computing the loss and updating the weight. When the losses become indistinguishable, we consider finding the minimalized loss and thus terminate the loop.\nBased on the regular gradient descent model, we implement one of its key variants, the stochastic gradient descent with an optional momentum feature. In the fit_stochasitic() function, I compute the stochastic gradient by picking a random subset and computing the corresponding gradient. Specifically, we use a nested for loop. The outer loop iterates through the max number of epochs input by the user by shuffling them first. The inner loop iterates through arrays of equal size indicated by the user. Each array is a batch. Within each batch, we calculate the stochastic gradient and conduct the update.\n\n\nPerformance Check\nInitiate autoreload and import packages.\n\n%load_ext autoreload\n%autoreload 2\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport random\n\nfrom gradient import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}\n\n\nGenerate a 2D dataset.\n\nnp.random.seed(300)\n# make the data\nX, y = make_blobs(n_samples = 200, n_features = 2, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nFit the model for regular gradient and check history.\n\n# fit the model\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.01, max_epochs = 1000)\n\n# inspect the fitted value of w\nLR.w \n\n# check history\nprint(f\"{LR.score_history[-10:] = }\")\nprint(f\"{LR.loss_history[-10:] = }\")\n\nLR.score_history[-10:] = [0.93, 0.93, 0.93, 0.93, 0.93, 0.93, 0.93, 0.93, 0.93, 0.93]\nLR.loss_history[-10:] = [0.23377412203767803, 0.2337256570612906, 0.2336772939128487, 0.2336290322835982, 0.2335808718660169, 0.2335328123538083, 0.2334848534418953, 0.23343699482641447, 0.23338923620470986, 0.2333415772753273]\n\n\nSeems right. Now we fit the models for stochasitc gradient (with momentum), stochasitc gradient (no momentum), and regular gradient. Then, we plot all three on the loglog plot to see how they converge over iterations.\n\nnp.random.seed(666)\n# stochasitc gradient (with momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = .05) \n\n# check history\nprint(f\"{LR.loss_history[-10:] = }\")\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\n# stochasitc gradient (no momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .05)\n# check history\nprint(f\"{LR.loss_history[-10:] = }\")\n\n# plot\nnum_steps = len(LR.loss_history)\nprint(num_steps)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n# gradient\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 100)\n\n# check history\nprint(f\"{LR.loss_history[-10:] = }\")\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\nprint(f\"{num_steps = }\")\n\n# plot loglog\nplt.loglog()\n\nlegend = plt.legend()\n\nLR.loss_history[-10:] = [0.19729864650198484, 0.19728844411058305, 0.19727935468954463, 0.19727046593738962, 0.19725968767836846, 0.19725286746340295, 0.19724485117413124, 0.19723713411086885, 0.1972312121585486, 0.19722350545933254]\nLR.loss_history[-10:] = [0.19732937184575206, 0.19731855298247097, 0.1973095909859817, 0.1973046706827699, 0.19729220743287826, 0.19728488395894772, 0.1972763999564443, 0.19726826759154018, 0.19726040304512601, 0.19725317650122887]\n100\nLR.loss_history[-10:] = [0.2790618158178214, 0.278179397253642, 0.2773132078315364, 0.2764628136516567, 0.27562779596145764, 0.27480775050780504, 0.2740022869216691, 0.2732110281335338, 0.2724336098177659, 0.271669679864307]\nnum_steps = 100\n\n\n\n\n\nAll three models converge given small enough learning rate. In general, the regular gradient model converges slower than the stochastic gradient models and might need more epochs to find a good solution.\n\n\nExperiments\nIt’s time to play with the parameters even more and see how the learning rate, batch size, momentum, and feature numbers!\n\nA case in which gradient descent does not converge to a minimizer because the learning rate is too large.\n\nWhen the learning rate (step size) is too large, as shown below, the model does not converge any more but oscillates instead.\n\nnp.random.seed(306)\n#gradient\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 100, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\nprint(f\"{num_steps = }\")\n\n# plot loglog\nplt.loglog()\n\nlegend = plt.legend()\n\nnum_steps = 100\n\n\n\n\n\n\nA case in which the choice of batch size influences how quickly the algorithm converges.\n\nThe smaller the batch, the more quickly the algorithm converges.\n\nnp.random.seed(307)\n# stochasitc gradient (no momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 5, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (batch size = 5)\")\n\n# stochasitc gradient (no momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\n#print(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (batch size = 10)\")\n\n# plot loglog\nplt.loglog()\n\nlegend = plt.legend()\n\n64\n100\n\n\n\n\n\n\nA case in which the use of momentum significantly speeds up convergence.\n\nI tried multiple random seeds, while sometimes the stochastic gradient models with and without momentum display similar converging rate, the one with momentum sometimes out perform the one without. Therefore, in general, we can conclude that the use of momentum significantly speeds up convergence.\n\nnp.random.seed(312)\n# make the data (10 features)\nX, y = make_blobs(n_samples = 200, n_features = 2, centers = [(-1, -1), (1, 1)])\n\n# stochasitc gradient (with momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X2, y2, \n                  m_epochs = 100, \n                  momentum = True, \n                  batch_size = 4, \n                  alpha = .07) \n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\n# print(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\n# stochasitc gradient (no momentum)\nLR = LogisticRegression()\nLR.fit_stochastic(X2, y2, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 4, \n                  alpha = .07)\n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\n# print(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n# plot loglog\nplt.loglog()\n\nlegend = plt.legend()\n\n27\n36"
  },
  {
    "objectID": "posts/penguin_classification/penguin.html",
    "href": "posts/penguin_classification/penguin.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "# Import\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom warnings import simplefilter\nfrom sklearn.exceptions import ConvergenceWarning\nsimplefilter(\"ignore\", category=ConvergenceWarning) # avoid warning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import GridSearchCV\nfrom matplotlib.patches import Patch\nfrom itertools import combinations\nimport seaborn as sns"
  },
  {
    "objectID": "posts/penguin_classification/penguin.html#data-inspection",
    "href": "posts/penguin_classification/penguin.html#data-inspection",
    "title": "Classifying Palmer Penguins",
    "section": "Data Inspection",
    "text": "Data Inspection\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN"
  },
  {
    "objectID": "posts/penguin_classification/penguin.html#data-prep",
    "href": "posts/penguin_classification/penguin.html#data-prep",
    "title": "Classifying Palmer Penguins",
    "section": "Data Prep",
    "text": "Data Prep\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train.head()\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n\n\n\n\ny_train\n\narray([2, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 2, 1, 2, 2, 0, 0, 1,\n       2, 2, 0, 2, 0, 1, 1, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n       1, 0, 2, 2, 2, 1, 1, 1, 2, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 1, 0, 0,\n       2, 0, 2, 2, 0, 2, 0, 0, 2, 1, 1, 2, 2, 0, 1, 2, 2, 2, 1, 0, 1, 0,\n       0, 0, 0, 1, 2, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 1, 0, 2, 0, 2, 0, 2,\n       0, 0, 2, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0, 2, 2,\n       0, 2, 2, 0, 1, 2, 1, 2, 0, 0, 0, 2, 0, 0, 1, 1, 0, 2, 1, 2, 2, 2,\n       2, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 2, 0, 2, 0, 1, 0, 1, 2, 1, 1, 1,\n       2, 0, 1, 0, 0, 1, 0, 0, 2, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 2, 1, 2,\n       0, 1, 2, 0, 2, 2, 1, 1, 0, 2, 0, 0, 2, 1, 0, 2, 2, 1, 1, 2, 2, 2,\n       0, 2, 2, 2, 2, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 1, 0, 0, 0, 0, 1, 2,\n       0, 1, 1, 0, 2, 0, 1, 1, 1, 0, 2, 0, 2, 2])"
  },
  {
    "objectID": "posts/penguin_classification/penguin.html#data-exploration",
    "href": "posts/penguin_classification/penguin.html#data-exploration",
    "title": "Classifying Palmer Penguins",
    "section": "Data Exploration",
    "text": "Data Exploration\nIn the following table, we group the data by sex and calculate the mean body mass for each sex. We can see from the result that the mean body mass of the male penguins is substantially higher than the mean body mass of the female penguins.\n\nX_train.groupby(['Sex_MALE'])[['Body Mass (g)']].aggregate([np.mean,len ]).round(2)\n\n\n\n\n\n  \n    \n      \n      Body Mass (g)\n    \n    \n      \n      mean\n      len\n    \n    \n      Sex_MALE\n      \n      \n    \n  \n  \n    \n      0\n      3823.21\n      126\n    \n    \n      1\n      4613.08\n      130\n    \n  \n\n\n\n\nIn the subsequent pair chart, we inspect the correlation between each pair of features. We can see that the culmen length, the culmen depth, the flipper length, and the body mass are in general positively correlated with each other. It also seems like if we were to differentiate the species, Delta 15 N would be a good measurement because the means of the three clusters are more distinctly separated compared to that separation in other features.\n\nsns.set_theme(style=\"ticks\")\nsns.pairplot(train, hue=\"Species\")\n\n<seaborn.axisgrid.PairGrid at 0x7f7f9fd9da00>"
  },
  {
    "objectID": "posts/penguin_classification/penguin.html#choosing-features",
    "href": "posts/penguin_classification/penguin.html#choosing-features",
    "title": "Classifying Palmer Penguins",
    "section": "Choosing Features",
    "text": "Choosing Features\nIn the following chunk of code, we loop through three models (logistic regression, support vector machine, and random forest) as well as the groups of features to determine what are the best three features (2 quantitative and 1 qualitative) to determine the species. The skeleton of the code is based on the blog instruction (https://middlebury-csci-0451.github.io/CSCI-0451/assignments/blog-posts/blog-post-penguins.html#your-challenge).\n\n# Scale the model to prevent warnings\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\n\n\n# Search for the best columns witht the largest cross validation score\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\n# Create a dictionary to store the models of interest\nmodels = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"SVC\": SVC(gamma = 0.01),\n    \"Random Forest\": RandomForestClassifier()  \n}\n\n# Initiate an empty array for the best cols, the score, and the best model\nbest_cols = None\nbest_score = 0\nbest_model = None\n\n# Main loop to iterate through the feature groups\nfor model_name, model in models.items():\n    print(f\"Evaluating {model_name}...\")\n    for qual in all_qual_cols: \n        qual_cols = [col for col in X_train.columns if qual in col]\n        for pair in combinations(all_quant_cols, 2):\n            cols =  list(pair) + qual_cols\n            # print(cols)\n            X_subset = X_train[cols]\n            scores = cross_val_score(model, X_subset, y_train, cv=5) # You can modify the cv parameter as needed\n            mean_score = scores.mean()\n            if mean_score > best_score:\n                best_cols = cols\n                best_score = mean_score\n                best_model = model_name\n            \n    print(f\"The best score for {model_name} was {best_score} and the best columns were {best_cols}\")\n    \nprint(f\"The best model was {best_model} with a score of {best_score} and the best columns were {best_cols}\")\n\nEvaluating Logistic Regression...\nThe best score for Logistic Regression was 0.996078431372549 and the best columns were ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nEvaluating SVC...\nThe best score for SVC was 0.996078431372549 and the best columns were ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nEvaluating Random Forest...\nThe best score for Random Forest was 0.996078431372549 and the best columns were ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nThe best model was Logistic Regression with a score of 0.996078431372549 and the best columns were ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']"
  },
  {
    "objectID": "posts/penguin_classification/penguin.html#model-choices",
    "href": "posts/penguin_classification/penguin.html#model-choices",
    "title": "Classifying Palmer Penguins",
    "section": "Model Choices",
    "text": "Model Choices\n\nLogistic Regression\n\n# Train/Fit & Calculate training score\nLR = LogisticRegression()\nLR.fit(X_train[best_cols], y_train)\nLR_train_score = LR.score(X_train[best_cols], y_train)\nprint(f\"{LR_train_score=}\")\n\nLR_train_score=1.0\n\n\n\n\nOther Models\n\nDecision Tree\n\n# Use cross validation to find the best depth and plug back to the \"Choosing Features\" chunk\nnp.random.seed(12345)\n\nfig, ax = plt.subplots(1)\n\nfor d in range(2, 5):\n    T = DecisionTreeClassifier(max_depth = d)\n    m = cross_val_score(T, X_train[cols], y_train, cv = 10).mean()\n    ax.scatter(d, m, color = \"black\")\n    # ax.scatter(d, T.score(X_test, y_test), color = \"firebrick\")\n\nlabs = ax.set(xlabel = \"Complexity (depth)\", ylabel = \"Performance (score)\")\n\n\n\n\n\n# Train/Fit & Calculate training score\nDTC = DecisionTreeClassifier(max_depth = 3)\nDTC.fit(X_train[best_cols], y_train)\nDTC_train_score = DTC.score(X_train[best_cols], y_train)\nprint(f\"{DTC_train_score=}\")\n\n# Show the decision tree plot\nplt.figure(figsize=(12,12))\np = plot_tree(DTC, feature_names = best_cols, filled = True, class_names = species, fontsize=8)\nplt.show()\n\nDTC_train_score=0.9921875\n\n\n\n\n\n\n\nRandom Forest\n\n# Train/Fit & Calculate training score\nrf = RandomForestClassifier()\nrf.fit(X_train[best_cols], y_train)\nrf_train_score = rf.score(X_train[best_cols], y_train)\nprint(f\"{rf_train_score=}\")\n\nrf_train_score=1.0\n\n\n\n\nSVC\n\n# Use grid search to find the best gamma and plug back to the \"Choosing Features\" chunk\nparam_grid = {'gamma': np.logspace(-5, 5, num=11)}\n\nsvc = SVC()\ngrid_search = GridSearchCV(svc, param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\n\nprint(\"Best gamma:\", grid_search.best_params_['gamma'])\nprint(\"Best score:\", grid_search.best_score_)\n\nBest gamma: 0.01\nBest score: 0.7730769230769232\n\n\n\n# Train/Fit & Calculate training score\nsvc_best = SVC(gamma = 0.01)\nsvc_best.fit(X_train[best_cols], y_train)\nsvc_train_score = svc_best.score(X_train[best_cols], y_train)\nprint(f\"{svc_train_score=}\")\n\nsvc_train_score=0.98046875"
  },
  {
    "objectID": "posts/penguin_classification/penguin.html#testing",
    "href": "posts/penguin_classification/penguin.html#testing",
    "title": "Classifying Palmer Penguins",
    "section": "Testing",
    "text": "Testing\n\n# Get the test data\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\n# LR\nLR.fit(X_train[best_cols], y_train)\nLR_score = LR.score(X_test[best_cols], y_test)\nprint(\"LR test score:\", LR_score)\n\n# Decision Tree\nDTC.fit(X_train[best_cols], y_train)\nDTC_score = DTC.score(X_test[best_cols], y_test)\nprint(\"Decision Tree test score:\", DTC_score)\n\n# Random Forest\nrf = RandomForestClassifier()\nrf.fit(X_train[best_cols], y_train)\nrf_score = rf.score(X_test[best_cols], y_test)\nprint(\"Random Forest test score:\", rf_score)\n\n# SVC\nsvc_best = SVC(gamma=0.01)\nsvc_best.fit(X_train[best_cols], y_train)\nsvc_score = svc_best.score(X_test[best_cols], y_test)\nprint(\"SVC test score:\", svc_score)\n\nLR test score: 1.0\nDecision Tree test score: 0.9852941176470589\nRandom Forest test score: 0.9852941176470589\nSVC test score: 0.9264705882352942"
  },
  {
    "objectID": "posts/penguin_classification/penguin.html#plotting-decision-regions",
    "href": "posts/penguin_classification/penguin.html#plotting-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "Plotting Decision Regions",
    "text": "Plotting Decision Regions\n\n# The skeleton of the code is based on the blog instruction (https://middlebury-csci-0451.github.io/CSCI-0451/assignments/blog-posts/blog-post-penguins.html#your-challenge).\ndef plot_regions(model, X, y, model_name):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    # add the title\n    fig.suptitle(model_name + \" Decision Boundaries\", fontsize=14)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n    \n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nDecision Boundaries for Training Data\n\nplot_regions(LR, X_train[best_cols], y_train, model_name = \"LR\")\nplot_regions(DTC, X_train[best_cols], y_train, model_name = \"DTC\")\nplot_regions(rf, X_train[best_cols], y_train, model_name = \"Random Forest\")\nplot_regions(svc_best, X_train[best_cols], y_train, model_name = \"Support Vector Machine\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Boundaries for Testing Data\n\nplot_regions(LR, X_test[best_cols], y_test, model_name = \"LR\")\nplot_regions(DTC, X_test[best_cols], y_test, model_name = \"DTC\")\nplot_regions(rf, X_test[best_cols], y_test, model_name = \"Random Forest\")\nplot_regions(svc_best, X_test[best_cols], y_test, model_name = \"Support Vector Machine\")"
  },
  {
    "objectID": "posts/linear_reg/linear_reg.html",
    "href": "posts/linear_reg/linear_reg.html",
    "title": "Implementing Linear Regression",
    "section": "",
    "text": "Link to the source code: https://github.com/yanruoz/yanruoz.github.io/blob/main/posts/linear_reg/linear_reg.py"
  },
  {
    "objectID": "posts/linear_reg/linear_reg.html#experiments",
    "href": "posts/linear_reg/linear_reg.html#experiments",
    "title": "Implementing Linear Regression",
    "section": "Experiments",
    "text": "Experiments\nIn the experiment below, we plot the change in the training and validation scores as the number of features increases and the number of training data points remains constant (100). We see that initially, the increase in feature number leads to a higher score overall. However, as the feature number increases to around 90, the score decreases sharply. As it approaches the number of training points, despite fluctuations, it follows a decreasing trend to negative numbers, indicating poor modeling. This phenomenon illustrates that overfitting might lead to worse performance.\n\n# create n_train and a list of p_features\nnp.random.seed(1234)\nn_train = 100\nn_val = 100\np_features_list = np.arange(1, n_train, 1).tolist() # step=1\n# print(p_features_list)\nnoise = 0.2\n\n# create empty lists to store training and validation scores\ntraining_score_list = []\nvalidation_score_list = []\n\nfor p_features in p_features_list:\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    \n    # fit the model\n    LR = LinearRegression()\n    LR.fit(X_train, y_train)\n    \n    # compute and append scores\n    training_score = LR.score(X_train, y_train).round(4)\n    validation_score = LR.score(X_val, y_val).round(4)\n    training_score_list.append(training_score)\n    validation_score_list.append(validation_score)\n\n# plot\nplt.plot(p_features_list, training_score_list, label = \"training score\")\nplt.plot(p_features_list, validation_score_list, label = \"validation score\")\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Score\")\nplt.legend(loc='best')\nplt.show\n\n<function matplotlib.pyplot.show(close=None, block=None)>"
  },
  {
    "objectID": "posts/linear_reg/linear_reg.html#lasso-regularization",
    "href": "posts/linear_reg/linear_reg.html#lasso-regularization",
    "title": "Implementing Linear Regression",
    "section": "LASSO Regularization",
    "text": "LASSO Regularization\n\nImplementation\n\n# use lasso regularization\nL = Lasso(alpha = 0.001)\n\n# fit the model on data\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL.fit(X_train, y_train)\n\nLasso(alpha=0.001)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=0.001)\n\n\n\n# compute the score\nL.score(X_val, y_val)\n\n0.7549598289478557\n\n\n\n\nExperiments\nWe implement the same experiments as previously did with linear regression, increasing the number of features up to or even past n_train - 1 (In this case, n_train=100, and I increased p_features to 150). The plots show the change of score as the number of features increases, with different values for the regularization strenth alpha in each subplot.\n\n# create n_train and a list of p_features\nnp.random.seed(1234)\nn_train = 100\nn_val = 100\np_features_list = np.arange(1, n_train + 50, 1).tolist()\n# print(p_features_list)\nnoise = 0.2\n\n\n# set subplot parameters\nfig, axarr = plt.subplots(2, 2)\nfig.suptitle('Comparison between different alphas in LASSO regularization')\nplt.rcParams[\"figure.figsize\"] = (10, 4)\n\n\n# initiate alpha list & axis array list\nalpha_list = [0.1, 0.01, 0.001, 0.0001]\naxarr_list = [(0,0), (0,1), (1,0), (1,1)]\n\n# draw subplots\nfor i in range(4):\n    \n    # create empty lists to store training and validation scores\n    training_score_list = []\n    validation_score_list = []\n    \n    for p_features in p_features_list:\n        # create data\n        X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n        # fit the model\n        L = Lasso(alpha = alpha_list[i])\n        L.fit(X_train, y_train)\n\n        # compute and append scores\n        training_score = L.score(X_train, y_train).round(4)\n        validation_score = L.score(X_val, y_val).round(4)\n        training_score_list.append(training_score)\n        validation_score_list.append(validation_score)\n    \n    # label, axis, and plot\n    axarr[axarr_list[i]].set_title(\"alpha = \" + str(alpha_list[i]))\n    axarr[axarr_list[i]].set(xlabel = \"Number of Features\", ylabel = \"Score\")\n    l1 = axarr[axarr_list[i]].axvline(x = n_train, ymin = -0.3, ymax = 1.1, linestyle = 'dashed', label = \"n_train\")\n    l2 = axarr[axarr_list[i]].plot(p_features_list, training_score_list, label = \"training score\")\n    l3 = axarr[axarr_list[i]].plot(p_features_list, validation_score_list, label = \"validation score\")\n    \nlabels = [\"n_train\", \"training score\", \"validation score\"]\nfig.legend([l1, l2, l3], labels=labels, loc=\"upper right\")   \nplt.subplots_adjust(hspace=.9)    \n\n\n\n\nAs alpha becomes smaller, the model tends to reach a better score and reach it more quickly. The score is also significantly higher compared to the linear regression. However, the effect of overfitting remains when the number of features is too large."
  },
  {
    "objectID": "posts/allocative_bias/bias.html",
    "href": "posts/allocative_bias/bias.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "#import\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\n\nSTATE = \"AL\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n  \n    \n      \n      RT\n      SERIALNO\n      DIVISION\n      SPORDER\n      PUMA\n      REGION\n      ST\n      ADJINC\n      PWGTP\n      AGEP\n      ...\n      PWGTP71\n      PWGTP72\n      PWGTP73\n      PWGTP74\n      PWGTP75\n      PWGTP76\n      PWGTP77\n      PWGTP78\n      PWGTP79\n      PWGTP80\n    \n  \n  \n    \n      0\n      P\n      2018GQ0000049\n      6\n      1\n      1600\n      3\n      1\n      1013097\n      75\n      19\n      ...\n      140\n      74\n      73\n      7\n      76\n      75\n      80\n      74\n      7\n      72\n    \n    \n      1\n      P\n      2018GQ0000058\n      6\n      1\n      1900\n      3\n      1\n      1013097\n      75\n      18\n      ...\n      76\n      78\n      7\n      76\n      80\n      78\n      7\n      147\n      150\n      75\n    \n    \n      2\n      P\n      2018GQ0000219\n      6\n      1\n      2000\n      3\n      1\n      1013097\n      118\n      53\n      ...\n      117\n      121\n      123\n      205\n      208\n      218\n      120\n      19\n      123\n      18\n    \n    \n      3\n      P\n      2018GQ0000246\n      6\n      1\n      2400\n      3\n      1\n      1013097\n      43\n      28\n      ...\n      43\n      76\n      79\n      77\n      80\n      44\n      46\n      82\n      81\n      8\n    \n    \n      4\n      P\n      2018GQ0000251\n      6\n      1\n      2701\n      3\n      1\n      1013097\n      16\n      25\n      ...\n      4\n      2\n      29\n      17\n      15\n      28\n      17\n      30\n      15\n      1\n    \n  \n\n5 rows × 286 columns\n\n\n\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      RAC1P\n      ESR\n    \n  \n  \n    \n      0\n      19\n      18.0\n      5\n      17\n      2\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      1\n      6.0\n    \n    \n      1\n      18\n      18.0\n      5\n      17\n      2\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      2\n      6.0\n    \n    \n      2\n      53\n      17.0\n      5\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      2\n      1\n      2\n      2\n      1.0\n      1\n      1\n      6.0\n    \n    \n      3\n      28\n      19.0\n      5\n      16\n      2\n      NaN\n      1\n      1.0\n      2.0\n      1\n      1\n      2\n      2\n      2.0\n      1\n      1\n      6.0\n    \n    \n      4\n      25\n      12.0\n      5\n      16\n      1\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      1.0\n      2\n      1\n      6.0\n    \n  \n\n\n\n\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n\nfor obj in [features, label, group]:\n    print(obj.shape)\n\n(47777, 15)\n(47777,)\n(47777,)"
  },
  {
    "objectID": "posts/linear_reg/linear_reg.html#implement-linear-regression-two-ways",
    "href": "posts/linear_reg/linear_reg.html#implement-linear-regression-two-ways",
    "title": "Implementing Linear Regression",
    "section": "Implement Linear Regression Two Ways",
    "text": "Implement Linear Regression Two Ways\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n# import packages\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom linear_reg import LinearRegression\nfrom sklearn.linear_model import Lasso\n\n\n# generate dataset\nnp.random.seed(1234)\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\n# plot dataset\n\n# set variables and parameters\nnp.random.seed(1234)\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nAfter obtaining the dataset, we firstly fit the model with the anlytic method and then inspect the training score, the validation score, and the weight.\n\n# analytic method\nnp.random.seed(1234)\n\nLR = LinearRegression()\nLR.fit(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\nLR.w\n\nTraining score = 0.2117\nValidation score = 0.3233\n\n\narray([0.36147991, 0.83389459])\n\n\nThen, we fit the model with the regular gradient and inspect the weight. We can see that the optimized weights of the two methods are quite close.\n\n# regular gradient\nnp.random.seed(1234)\nLR_reg = LinearRegression()\nLR_reg.fit_gradient(X_train, y_train, alpha = 0.001, max_epochs = 1000)\nLR_reg.w\n\narray([0.34744356, 0.84172573])\n\n\nLastly, we implement the stochastic gradient and still get a similar final weight.\n\n# stochastic gradient\nnp.random.seed(1234)\nLR_sto = LinearRegression()\nLR_sto.fit(X_train, y_train, method = 'gradient', alpha = 0.001, m_epochs = 1000)\nLR_sto.w\n\narray([0.35754189, 0.83609168])\n\n\nIn the following graphs, we see how the score changed over time for the regular and the stochastic gradient descent respectively.\n\nplt.plot(LR_reg.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\nplt.title(\"The Change of Score for Regular Gradient Descent\")\n\nText(0.5, 1.0, 'The Change of Score for Regular Gradient Descent')\n\n\n\n\n\n\nplt.plot(LR_sto.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\nplt.title(\"The Change of Score for Stochastic Gradient Descent\")\n\nText(0.5, 1.0, 'The Change of Score for Stochastic Gradient Descent')"
  }
]