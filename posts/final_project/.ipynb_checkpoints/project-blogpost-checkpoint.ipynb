{
 "cells": [
  {
   "cell_type": "raw",
   "id": "efd265a0-a772-451f-950d-4d7276eec5a5",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Speech Emotion Recognition\"\n",
    "date: 2023-05-14\n",
    "description: |\n",
    "    Demonstrating the motivation, methods, and results of our final project, speech emotion recognition. \n",
    "publish: \"true\"\n",
    "bibliography: refs.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278f3d80-a776-4b77-a645-7fa7c5f70885",
   "metadata": {},
   "source": [
    "# Speech Emotion Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df669a1-ce3e-48c5-8817-50d0c8d87598",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d18c9b-5b32-4411-b392-bbae379060ff",
   "metadata": {},
   "source": [
    "Our project focused on predicting the emotion expressed by human speech based on the Mel Frequency Cepstral Coefficient (MFCC) features. Using the speech audio files in the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), we approached the problem by cleaning the data and transforming the audio files to a machine-readable format, visually exploring the data with a focus on the MFCC features in different emotions, experimenting with 1D and 2D convolutional neural network (CNN) models to train and test. We achieved the highest training accuracy of 78% and test accuracy of 66% via our 1D CNN model with two convolutional layers, two fully-connected layers, and 30 MFCCs, which is higher than the base rate but lower than existing models with a similar approach. In the future, we hope to work on experimenting with other deep learning models, using different numbers of MFCC features, increasing the number of epochs, and applying data augmentation to improve our results. By providing an ethical reflection on speech emotion recognition, we acknowledged the controversiality of this field and urged researchers to ponder on the groups that potentially benefit from and are harmed by related works. \n",
    "\n",
    "Our code is available at: https://github.com/wendianaxu/speech-emotion-recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f9ff9-a55b-4e6e-a1ce-0e59bbc839ca",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Recognizing emotions from human speech is an important task with various applications in human-centered computing, because it allows automatic systems to interpret users’ emotions, and make decisions and provide responses accordingly. In real life, people convey emotions not only through the literal meanings of their speech, but also, and probably more often, with voice qualities such as intonation. Accurate speech emotion recognition with machines has the potential to benefit fields such as criminal investigation, medical care, and the service industry. \n",
    "\n",
    "Many researchers have tackled the problem of speech emotion recognition with machine learning models. @badshah2017speech used Convolutional Neural Networks (CNN) on speech spectrogram images to classify seven emotions and achieved an overall accuracy of 84.3% across all data. However, emotions of boredom, fear, happiness, and neutral have relatively low accuracies below 50%. \n",
    "\n",
    "Mel-Frequency Cepstral Coefficients (MFCCs) are coefficients that represent an audio clip and contain information about the rate changes in the different spectrum bands. @xu2020improve used MFCCs as features to train an attention-based CNN model on speech emotion data. Their model achieved an accuracy of around 76% in classifying speech into nine emotions. @kumbhar2019speech used MFCC features on Long Short Term Memory (LSTM) models and obtained an accuracy of around 85% with the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), which is the dataset that we used for our project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f479b14-be05-4542-986b-9ad2af349e06",
   "metadata": {},
   "source": [
    "## Values Statement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65176f69-c02d-43b8-a1fd-73ec08f0618b",
   "metadata": {},
   "source": [
    "Audio speech emotion recognition is a sub-branch of speech recognition, which plays an important role in deepening and widening the AI’s functionality. Potential users of speech emotion recognition include people from commercial industries, the medical field, and the anti-criminal field. \n",
    "\n",
    "In the commercial industry, for example, a developed speech emotion detection and classification technology can help dialogue systems for spoken language such as call centers, onboard vehicle driving systems, and intelligent voice systems like Siri to better understand human emotions, leading to improved human-computer interactions and a more customized user experience. Similarly, applications and websites utilizing AI for text-to-speech conversions could cater to visually impaired users by conveying emotions effectively.\n",
    "\n",
    "Beyond commercial applications, the medical field can also benefit from the application of our project. Emotion classification could serve as a valuable tool for various aspects of the field of mental health. Firstly, it could contribute potentially to assisting the diagnosis of psychiatric disorders. If audio-speech detection systems could accurately detect and categorize emotions expressed by individuals, clinicians, and mental health professionals might gain deeper insights into their psychological well-being, potentially leading to more accurate diagnoses and personalized treatment plans. Additionally, emotion classification can play a vital role in offering effective psychological counseling. By analyzing emotional patterns and fluctuations over time, therapists can gain a better understanding of their clients’ emotional states as the sessions go on, thus tailoring their counseling approaches accordingly. Furthermore, the ability to monitor mental states using emotion classification technology holds great promise in preventing potential risks of mental illnesses and speech-related disorders. By continuously assessing the emotions of an individual, it increases the probability of detecting early warning signs of heightened emotional distress, and deteriorating mental conditions, proactively allowing timely interventions and preventive measures. Emotional cues and patterns in speech can provide valuable insights into certain conditions such as hyperkinetic dysphonia and hypokinetic dysphonia. Therefore, by accurately recognizing and classifying these emotional nuances in speech, clinicians may better assess speech disorders, catch the early symptoms, and provide more targeted treatment plans. In summary,  serving as a potentially effective tool in diagnosing psychiatric diseases, offering psychological counseling, monitoring mental states to prevent risks, and identifying speech-related disorders, speech emotion classification has wide-ranging applications in the medical field, contributing to the technological revolution for providing better medical care.\n",
    "\n",
    "Another field with potential users is the realm of crime prevention and resolution. The application of our project could potentially contribute to the development of lie-detection algorithms and devices. The ability to accurately detect deception can have profound implications for law enforcement agencies, intelligence organizations, and legal proceedings. By leveraging the advancements in audio emotion detection and classification, the development of robust lie detection algorithms and devices can empower law enforcement agencies and negotiators with an additional layer of objective analysis, complementing their expertise and intuition. Despite the challenges of ethically incorporating this technology into the legal framework, the inclusion of emotion and lie detection has great potential to augment the capabilities of people who work in the field of anti-criminality.\n",
    "\n",
    "However, certain groups may not directly benefit from this project. Hearing-disabled individuals, who predominantly rely on non-verbal communication, are excluded from this kind of technology. Greater risks of the application of this project lie in its use for non-ethical deeds. For instance, kidnappers could exploit emotion detection algorithms to gauge the emotions of hostages and their families, potentially facilitating blackmail.\n",
    "We must also address potential biases in this project and its applications. Since the dataset primarily employs English with a neutral North American accent, the resulting models may be biased against speakers with different accents or non-English speakers. In commercial products, it may not work as well for users who are non-English speakers or possess certain dialects. Since accents and languages suggest a difference in cultural, racial, and perhaps even socio-economic status, the careless use of this technology before enough diverse data are collected could further harm the already marginalized groups. This misuse could cause substantial harm especially if used in the medical or the anti-criminal field for making important decisions such as hospitalization or sentencing. Nonetheless, starting with a \"standard\" English pipeline and acknowledging the potential biases and dire consequences is not a bad way to start developing a technology. However, to foster inclusivity, it is crucial to gradually incorporate more dialects and languages, ensuring the development of comprehensive speech-emotion recognition algorithms.\n",
    "\n",
    "Our personal reasons to work on this problem of implementing, improving, and spreading the technology of speech emotion recognition emerged from our interest in psychology and human-computer interaction. However, as we consider more the ethical controversies of this project and its applications, we are infused with the sense of responsibility to acknowledge the potential biases and make an effort to proactively address them. Therefore, we are driven by our passion for the technological aspects but also the sense of awareness to bring our interest in the direction of benefiting the world and cautiously making technology inclusive to the marginalized groups. \n",
    "Given our discussion on the potential biases and harm the application of speech emotion recognition may bring, it is difficult to assert that the world will become a better place with more equity, justice, and care with this technology. Therefore, we remind ourselves and urge professionals working in the realm to be cautious and remain critical regarding our work, progressing towards eliciting this technology’s most beneficial potential and reducing the potential disadvantages as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92516d70-1163-49ea-8a84-f95ba8b65034",
   "metadata": {},
   "source": [
    "## Materials and Methods\n",
    "### Our Data\n",
    "Our dataset of speech emotions comes from Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), collected by @livingstone2018ryerson. The dataset contains vocalizations of only two different English sentences that share the same lexical structure. They were spoken by 24 actors, male and female, each in eight emotions of two levels of intensity. Each combination of sentence and emotion was repeated twice by the same actor. These vocalizations were validated by more than 200 raters who rated them on emotional validity, intensity, and genuineness. \n",
    "\n",
    "Since the dataset only contains English sentences spoken in a neutral North American accent, it has a limited scope in that it does not represent speakers of other languages and English speakers using other accents. \n",
    "\n",
    "### Our Approach\n",
    "We chose to use MFCC features as predictors for our models because they have shown good performance in previous works. We experimented with different numbers of MFCC features: 13 MFCCs, which is a common choice for speech recognition (@hasan2021many), and 30 MFCCs. We also experimented with taking the mean of MFCCs across time, resulting in an 1D feature matrix with only MFCCs, or including time as another dimension in our feature matrix. Our targets are the emotion labels from the original dataset. \n",
    "\n",
    "Because the audio files in the dataset contain silent moments, we trimmed out these moments to reduce irrelevant features. \n",
    "We used 1D CNN models on our aggregate MFCC features and 2D CNN models on MFCC features across time. We started from basic model architectures with two convolutional layers and two fully-connected layers, and adjusted them by adding layers according to training results. For example, we added dropout layers for models that exhibited overfitting. \n",
    "\n",
    "We trained our models on GPU in Google Colab. We used cross entropy loss for our loss function and Adam for optimizer. Our learning rate was 0.001. \n",
    "\n",
    "We evaluated our models primarily using running training accuracy and testing accuracy. The size of our test set was 20% of the entire dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa10ea0e-e9c1-421f-9e77-d46688b910bf",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35f47e6-fa1e-43a2-afcf-d59adf3739cc",
   "metadata": {},
   "source": [
    "Among our 1D CNN models, the best model is the one with 2 convolutional layers, 2 fully-connected layers, and 30 MFCCs. This model has a training accuracy of 78% and a test accuracy of 66%, which although is not the most ideal result, still makes a much better prediction than the base rate of 12.5% (Table 1). The accuracy results of all our 1D CNN models are shown in Table 1.\n",
    "\n",
    "Since we found that the test accuracy increased as the number of MFCCs increased in our 1D CNN models, we only developed 2D models with 30 MFCCs (Table 1). Table 2 demonstrates the accuracies obtained from all of our 2D CNN models. In general, 2D CNN models illustrated a significant improvement in training accuracy but not test accuracy compared to the 1D models, and our best model remains to be the 1D model with 2 convolutional layers, 2 fully-connected layers, and 30 MFCCs (Table 2).\n",
    "\n",
    "From Figure 1, we see that as the number of features increases, while the training accuracy increases to reach a plateau, the validation accuracy remains fluctuating, indicating the problem of overfitting. The overfitting is especially obvious in our 2D models, so we attempted to adjust it by adding the dropout layer (Figure 1). The overfitting got slightly under control, which is illustrated by a drop in the training accuracy plateau and an increasing trend in the validation accuracy (Figure 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc650460-c15c-4882-ae05-7e1256c4ecd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"table1.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"table1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9103bce2-1ba6-4365-9255-cc8cd2870633",
   "metadata": {},
   "source": [
    "Table 1. The summary statistics of the 1D convolutional neural network (CNN) models. # MFCC  is the number of MFCCs. The # Conv layers is the number of convolutional layers. The # FC layers is the number of fully-connected layers. The learning rate for all the listed models is 0.001, and the number of epochs is 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b407f799-87fb-4673-a83d-175a8438e73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"table2.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"table2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c4b277-1ca4-4a6f-90fe-43460adf49bd",
   "metadata": {},
   "source": [
    "Table 2. The summary statistics of the 2D convolutional neural network (CNN) models. All models have 2 convolutional layers, 2 fully-connected layers, 30 MFCCs, and a learning rate of 0.001. The number of epochs are set to be 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51344405-cddb-4aa8-b820-c7765007a72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"fig1.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"fig1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69e3ac5-f191-4c6c-83da-6a4c3b347557",
   "metadata": {},
   "source": [
    "Figure 1. The change of training and validation accuracies over epochs on selected 1D and 2D CNN models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0969b9b2-8bb6-4bd9-b059-4a0f26ba7e53",
   "metadata": {},
   "source": [
    "## Concluding Discussion\n",
    "Our project is a complete process with the following planned steps: 1) cleaning the data and transforming the audio files to a machine-readable format, 2) exploring the data with a focus on the MFCC features, 3) using deep learning models to train the test dataset, and 4) testing with the test dataset. We have a notebook showing our working flow with detailed commenting and illustrative figures. \n",
    "\n",
    "We said in our proposal that our evaluation of the success will be based on: 1) completeness in data cleaning, training, and testing; 2) use of plots that demonstrate comprehensive data exploration and data analysis; 3) level of detail in documentation; 4) level of model accuracy; 5) depth of discussion on implications and potential biases; and 6) the clarity, conciseness, and accessibility of the presentation.\n",
    "\n",
    "We did a quite good job on criteria 1), 2), 3), and 5). For criterion 4), although we improved our models in the limited time, they still have many potential directions for improvement. Although our models achieved accuracies significantly higher than the base rate, they are less accurate than existing models that used similar approaches to recognize speech emotions, which typically have accuracies of 70-80%. If we had more time, we would do the following to improve our test accuracy:\n",
    "\n",
    "- Try to use another deep learning model such as the LSTM network, which was shown to work well with audio data (@kumbhar2019speech). \n",
    "- Experiment with different numbers of MFCC features. Our 2D models with 30 MFCCs showed significant overfitting, so we may be able to improve performance by reducing the number of MFCCs. \n",
    "- Use data augmentation techniques such as adding white noise, shifting, and stretching to increase the data size. \n",
    "- Train with more epochs, since the 2D model with the dropout layer seemed not reaching its plateau.\n",
    "\n",
    "For criteria 6), we had a clear and understandable presentation with assisting tables and figures however, we were a bit short on time to present the final slide on the discussion of ethics for our project, which is an important component of our project. If we had more time, we would also like to perform a bias audit on our results, for example on whether accuracies vary for male and female voices. \n",
    "\n",
    "In terms of the deliverables, our proposed success was to have a Python package and a Jupyter notebook. However, since we did not write any separate Python scripts, and everything was integrated into one notebook, we still consider that we have met the completeness goal. \n",
    "\n",
    "In terms of model quality, we said partial success is that despite the completeness, the model has a low accuracy. Since our model reached a much higher-than-base-rate accuracy that is lower than the accuracy achieved by publicly available algorithms, we think we fall between partial and full success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e08144-d708-4f67-a8ca-646fa3ea3c8e",
   "metadata": {},
   "source": [
    "## Group Contributions Statement\n",
    "### Code\n",
    "- Data preprocessing: Diana\n",
    "- Data exploration: Alice\n",
    "- 1D models: Diana & Alice\n",
    "- 2D models: Diana\n",
    "### Writing and Presentation\n",
    "- Abstract: Alice\n",
    "- Introduction: Diana\n",
    "- Values statement: Alice\n",
    "- Materials and methods: Diana\n",
    "- Concluding discussion: Diana & Alice\n",
    "- Group contributions statement: Diana & Alice\n",
    "- Presentation preparation and delivery: Diana & Alice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc35ef29-3abb-4cb7-ba86-7f6f8a2e3e34",
   "metadata": {},
   "source": [
    "## Personal Reflection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
