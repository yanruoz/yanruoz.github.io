{
 "cells": [
  {
   "cell_type": "raw",
   "id": "efd265a0-a772-451f-950d-4d7276eec5a5",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Speech Emotion Recognition\"\n",
    "date: 2023-05-14\n",
    "description: |\n",
    "    Demonstrating the motivation, methods, and results of our final project, speech emotion recognition. \n",
    "publish: \"true\"\n",
    "bibliography: refs.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278f3d80-a776-4b77-a645-7fa7c5f70885",
   "metadata": {},
   "source": [
    "# Speech Emotion Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df669a1-ce3e-48c5-8817-50d0c8d87598",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f9ff9-a55b-4e6e-a1ce-0e59bbc839ca",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Recognizing emotions from human speech is an important task with various applications in human-centered computing, because it allows automatic systems to interpret usersâ€™ emotions, and make decisions and provide responses accordingly. In real life, people convey emotions not only through the literal meanings of their speech, but also, and probably more often, with voice qualities such as intonation. Accurate speech emotion recognition with machines has the potential to benefit fields such as criminal investigation, medical care, and the service industry. \n",
    "\n",
    "Many researchers have tackled the problem of speech emotion recognition with machine learning models. @badshah2017speech used Convolutional Neural Networks (CNN) on speech spectrogram images to classify seven emotions and achieved an overall accuracy of 84.3% across all data. However, emotions of boredom, fear, happiness, and neutral have relatively low accuracies below 50%. \n",
    "\n",
    "@xu2020improve used Mel-Frequency Cepstral Coefficients (MFCC) as features to train an attention-based CNN model on speech emotion data. Their model achieved an accuracy of around 76% in classifying speech into nine emotions. @kumbhar2019speech used MFCC features on Long Short Term Memory (LSTM) models and obtained an accuracy of around 85% with the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), which is the dataset that we used for our project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f479b14-be05-4542-986b-9ad2af349e06",
   "metadata": {},
   "source": [
    "## Values Statement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92516d70-1163-49ea-8a84-f95ba8b65034",
   "metadata": {},
   "source": [
    "## Materials and Methods\n",
    "### Our Data\n",
    "Our dataset of speech emotions comes from Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), collected by @livingstone2018ryerson. The dataset contains vocalizations of only two different English sentences that share the same lexical structure. They were spoken by 24 actors, male and female, each in eight emotions of two levels of intensity. Each combination of sentence and emotion was repeated twice by the same actor. These vocalizations were validated by more than 200 raters who rated them on emotional validity, intensity, and genuineness. \n",
    "\n",
    "Since the dataset only contains English sentences spoken in a neutral North American accent, it has a limited scope in that it does not represent speakers of other languages and English speakers using other accents. \n",
    "\n",
    "### Our Approach\n",
    "We chose to use MFCC features as predictors for our models because they have shown good performance in previous works. We experimented with different numbers of MFCC features: 13 MFCCs, which is a common choice for speech recognition (@hasan2021many), and 30 MFCCs. We also experimented with taking the mean of MFCCs across time, resulting in an 1D feature matrix with only MFCCs, or including time as another dimension in our feature matrix. Our targets are the emotion labels from the original dataset. \n",
    "\n",
    "Because the audio files in the dataset contain silent moments, we trimmed out these moments to reduce irrelevant features. \n",
    "We used 1D CNN models on our aggregate MFCC features and 2D CNN models on MFCC features across time. We started from basic model architectures with two convolutional layers and two fully-connected layers, and adjusted them by adding layers according to training results. For example, we added dropout layers for models that exhibited overfitting. \n",
    "\n",
    "We trained our models on GPU in Google Colab. We used cross entropy loss for our loss function and Adam for optimizer. Our learning rate was 0.001. \n",
    "\n",
    "We evaluated our models primarily using running training accuracy and testing accuracy. The size of our test set was 20% of the entire dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa10ea0e-e9c1-421f-9e77-d46688b910bf",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0969b9b2-8bb6-4bd9-b059-4a0f26ba7e53",
   "metadata": {},
   "source": [
    "## Concluding Discussion\n",
    "Our project is a complete process with the following planned steps: 1) cleaning the data and transforming the audio files to a machine-readable format, 2) exploring the data with a focus on the MFCC features, 3) using deep learning models to train the test dataset, and 4) testing with the test dataset. We have a notebook showing our working flow with detailed commenting and illustrative figures. \n",
    "\n",
    "We said in our proposal that our evaluation of the success will be based on: 1) completeness in data cleaning, training, and testing; 2) use of plots that demonstrate comprehensive data exploration and data analysis; 3) level of detail in documentation; 4) level of model accuracy; 5) depth of discussion on implications and potential biases; and 6) the clarity, conciseness, and accessibility of the presentation.\n",
    "\n",
    "We did a quite good job on criteria 1), 2), 3), and 5). For criterion 4), although we improved our models in the limited time, they still have many potential directions for improvement. Although our models achieved accuracies significantly higher than the base rate, they are less accurate than existing models that used similar approaches to recognize speech emotions, which typically have accuracies of 70-80%. If we had more time, we would do the following to improve our test accuracy:\n",
    "\n",
    "- Try to use another deep learning model such as the LSTM network, which was shown to work well with audio data (@kumbhar2019speech). \n",
    "- Experiment with different numbers of MFCC features. Our 2D models with 30 MFCCs showed significant overfitting, so we may be able to improve performance by reducing the number of MFCCs. \n",
    "- Use data augmentation techniques such as adding white noise, shifting, and stretching to increase the data size. \n",
    "- Train with more epochs, since the 2D model with the dropout layer seemed not reaching its plateau.\n",
    "\n",
    "For criteria 6), we had a clear and understandable presentation with assisting tables and figures however, we were a bit short on time to present the final slide on the discussion of ethics for our project, which is an important component of our project. If we had more time, we would also like to perform a bias audit on our results, for example on whether accuracies vary for male and female voices. \n",
    "\n",
    "In terms of the deliverables, our proposed success was to have a Python package and a Jupyter notebook. However, since we did not write any separate Python scripts, and everything was integrated into one notebook, we still consider that we have met the completeness goal. \n",
    "\n",
    "In terms of model quality, we said partial success is that despite the completeness, the model has a low accuracy. Since our model reached a much higher-than-base-rate accuracy that is lower than the accuracy achieved by publicly available algorithms, we think we fall between partial and full success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e08144-d708-4f67-a8ca-646fa3ea3c8e",
   "metadata": {},
   "source": [
    "## Group Contributions Statement\n",
    "### Code\n",
    "- Data preprocessing: Diana\n",
    "- Data exploration: Alice\n",
    "- 1D models: Diana & Alice\n",
    "- 2D models: Diana\n",
    "### Writing and Presentation\n",
    "- Abstract: Alice\n",
    "- Introduction: Diana\n",
    "- Values statement: Alice\n",
    "- Materials and methods: Diana\n",
    "- Concluding discussion: Diana & Alice\n",
    "- Group contributions statement: Diana & Alice\n",
    "- Presentation preparation and delivery: Diana & Alice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc35ef29-3abb-4cb7-ba86-7f6f8a2e3e34",
   "metadata": {},
   "source": [
    "## Personal Reflection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
